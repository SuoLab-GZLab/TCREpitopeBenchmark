{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43163f7b",
   "metadata": {},
   "source": [
    "# 1.Original model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc9d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from scipy.special import expit\n",
    "import argparse\n",
    "def predict(cdrs,epitopes,labels,model,batch_size = 128):\n",
    "    batch_num_total = len(cdrs) // batch_size if len(cdrs) % batch_size == 0 else len(cdrs) // batch_size + 1\n",
    "    y_pres,y_trues ,epitope,y_preds_binary,CDR3B= [],[],[],[],[]\n",
    "    for batch_num in tqdm(range(batch_num_total)):\n",
    "        end = (batch_num+1)*batch_size if (batch_num+1)*batch_size <= len(cdrs) else len(cdrs)                \n",
    "        ts,es = cdrs[batch_num*batch_size:end],epitopes[batch_num * batch_size:end] \n",
    "        score = model(ts,es) \n",
    "        y_pres.extend(expit(score.view(-1).detach().cpu().numpy()))\n",
    "        y_trues.extend(labels[batch_num*batch_size : end])\n",
    "        epitope.extend(epitopes[batch_num*batch_size: end])\n",
    "        CDR3B.extend(cdrs[batch_num*batch_size: end])\n",
    "        threshold = 0.5\n",
    "        y_preds_binary = [1 if p >= threshold else 0 for p in y_pres]\n",
    "        df = pd.DataFrame({'epitope':epitope,'CDR3B': CDR3B, 'y_true': y_trues,'y_pred':y_preds_binary, 'y_prob': y_pres})\n",
    "        #name_str = '_'.join(name)\n",
    "        result_path1 = result_path +  'probability.csv'\n",
    "        pd.DataFrame(df).to_csv(result_path1)   \n",
    "    return {'epitope': epitope,'CDR3B': CDR3B,'y_true':y_trues,'y_prob':y_pres,'y_pred':y_preds_binary}\n",
    "\n",
    "class Args:\n",
    "    use_column='CDR3B'\n",
    "    batch_size=128\n",
    "  #  modelfile_path=modelfile_path\n",
    "    device='cuda:0'\n",
    "    def __init__(self, testfile_path, result_path,modelfile_path):\n",
    "        self.testfile_path = testfile_path \n",
    "        self.result_path = result_path\n",
    "        self.modelfile_path = modelfile_path\n",
    "        \n",
    "def Original_model_prediction(testfile_path,modelfile_path,result_path):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, matthews_corrcoef, precision_score,auc,roc_curve\n",
    "    args = Args(testfile_path=testfile_path, result_path=result_path, modelfile_path=modelfile_path)\n",
    "    f = pd.read_csv(args.testfile_path)\n",
    "    #load model\n",
    "    model_tcr = TCRpeg(hidden_size=768,num_layers = 3,load_data=False,embedding_path='encoders/aa_emb_tcr.txt',device=args.device)\n",
    "    model_tcr.create_model()\n",
    "    model_epi = TCRpeg(hidden_size=768,num_layers = 3,load_data=False,embedding_path='encoders/aa_emb_tcr.txt',device=args.device)\n",
    "    model_epi.create_model()\n",
    "    model = TEINet(en_tcr=model_tcr,en_epi = model_epi,cat_size=768*2,dropout=0.1,normalize=True,weight_decay = 0,device=args.device).to(args.device)\n",
    "    model.load_state_dict(torch.load(args.modelfile_path))\n",
    "    # model = torch.load(args.modelfile_path,map_location='cuda:0')\n",
    "    cdrs,epitopes,labels = f['CDR3B'].values, f['Epitope'].values, f['Affinity'].values\n",
    "    pres = predict(cdrs,epitopes,labels,model)\n",
    "    result = pd.DataFrame(pres)\n",
    "    result.to_csv(result_path+'probability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile_path=\"../data/test.csv\"\n",
    "modelfile_path=\"../Original_model/TEINet_large.pth\"\n",
    "result_path=\"../result_path/Original_model_prediction\"\n",
    "Original_model_prediction(testfile_path,modelfile_path,result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b223267",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile_path=\"../data/test.csv\"\n",
    "modelfile_path=\"../Original_model/TEINet_small.pth\"\n",
    "result_path=\"../result_path/Original_model_prediction\"\n",
    "Original_model_prediction(testfile_path,modelfile_path,result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859049f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daeb115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08966cb6",
   "metadata": {},
   "source": [
    "# 2.Model retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73b124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcrpeg.TCRpeg import TCRpeg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import average_precision_score as AUPRC\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sampler import Sampler\n",
    "from model import TEINet\n",
    "import psutil\n",
    "import time\n",
    "class Args:\n",
    "    dropout=0.0\n",
    "    epochs=3\n",
    "    cat_size=1536\n",
    "    batch_size=48\n",
    "    sample_num=10\n",
    "    info='information'\n",
    "    aa_tcr='encoders/aa_emb_tcr.txt'\n",
    "    aa_epi= 'encoders/aa_emb_epi.txt'\n",
    "    pretrain_tcr='encoders/encoder_tcr.pth'\n",
    "    pretrain_epi='encoders/encoder_epi.pth'\n",
    "    weight=4\n",
    "    weight_decay=0.0\n",
    "    normalize=1  \n",
    "    step1=21\n",
    "    step2=27\n",
    "    step3=30          \n",
    "    lr=0.001              \n",
    "    record_path='results/predictions.txt'   \n",
    "   # output_path='results/training.txt'\n",
    "  #  save_modle_path='None'\n",
    "   # trainfile_path='data/train_pos.csv'\n",
    "    static=0\n",
    "   # test_file='None'\n",
    "    pretrain=True\n",
    "   # sample_strategy='sample_epi'\n",
    "    def __init__(self,result_path, trainfile_path,save_modle_path,testfile_path):\n",
    "        self.result_path = result_path\n",
    "        self.trainfile_path = trainfile_path\n",
    "        self.testfile_path=testfile_path\n",
    "        self.save_modle_path=save_modle_path\n",
    "\n",
    "def Model_retraining(trainfile_path,testfile_path,save_modle_path,result_path):\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import shuffle\n",
    "    args = Args(result_path=result_path, trainfile_path=trainfile_path,save_modle_path=save_modle_path,testfile_path=testfile_path)  \n",
    "    normalize = False if args.normalize == 0 else True     \n",
    "    hidden_size = args.cat_size // 2 \n",
    "    cat_size = args.cat_size        \n",
    "    static = True if args.static == 1 else False\n",
    "    model_tcr = TCRpeg(hidden_size=hidden_size,num_layers = 3,load_data=False,embedding_path=args.aa_tcr)\n",
    "    #load pretrained TCR model\n",
    "    if args.pretrain: \n",
    "        print('using the pretrained model')\n",
    "        model_tcr.create_model(load=True,path=args.pretrain_tcr)\n",
    "    else :\n",
    "        model_tcr.create_model()\n",
    "    model_tcr.model.train()\n",
    "    model_epi = TCRpeg(hidden_size=hidden_size,num_layers = 3,load_data=False,embedding_path=args.aa_epi)\n",
    "    if args.pretrain:                             \n",
    "        model_epi.create_model(load=True,path = args.pretrain_epi)\n",
    "    else :\n",
    "        model_epi.create_model()\n",
    "    model_epi.model.train()\n",
    "    dropout = args.dropout\n",
    "    cat_size = args.cat_size\n",
    "    model = TEINet(en_tcr=model_tcr,en_epi = model_epi,cat_size=cat_size,dropout=dropout,normalize=normalize,weight_decay = args.weight_decay).to('cuda:0')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    if args.testfile_path != 'None':\n",
    "        data = pd.read_csv(args.testfile_path,low_memory=False)   \n",
    "        cs_test,es_test,ls_test = data['CDR3B'].values,data['Epitope'].values,data['Affinity'].values\n",
    "    pos_weight = (torch.ones([1])*args.weight).to('cuda:0')    \n",
    "    loss_fcn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    epochs = args.epochs\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    sample_num = args.sample_num      \n",
    "    record_aucs = []\n",
    "    train_data = pd.read_csv(args.trainfile_path,low_memory=False)   \n",
    "    cs_train,es_train,ls_train = train_data['CDR3B'].values,train_data['Epitope'].values,train_data['Affinity'].values\n",
    "    tcrs, epitopes, labels = shuffle(cs_train, es_train, ls_train, random_state=123)\n",
    "    infer = np.random.permutation(len(tcrs))\n",
    "    tcrs, epitopes, labels = tcrs[infer], epitopes[infer], labels[infer]\n",
    "    for e in range(epochs):\n",
    "        batch_num_total = len(tcrs) // batch_size if len(tcrs) % batch_size == 0 else len(tcrs) // batch_size + 1\n",
    "        infer = np.random.permutation(len(tcrs))\n",
    "        for batch_num in tqdm(range(batch_num_total)):\n",
    "            end = (batch_num+1)*batch_size if (batch_num+1)*batch_size <= len(tcrs) else len(tcrs)                \n",
    "            ts,es = tcrs[batch_num*batch_size:end],epitopes[batch_num * batch_size:end]             \n",
    "            ls = labels[batch_num*batch_size:end]                \n",
    "            ls = torch.FloatTensor(ls).to('cuda:0')  \n",
    "            output = model(ts,es)\n",
    "            if args.weight_decay == 0.0:\n",
    "                loss = loss_fcn(output.view(-1),ls)\n",
    "            else :\n",
    "                loss = loss_fcn(output[0].view(-1),ls) + output[1]   \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        y_pres = []\n",
    "        y_trues = []\n",
    "        CDR3B = []\n",
    "        epitope = []\n",
    "        y_preds_binary =[]\n",
    "        if args.testfile_path != 'None':\n",
    "            with torch.no_grad():\n",
    "                y_pres = []\n",
    "                y_trues = []\n",
    "                batch_num = len(cs_test) // batch_size if len(cs_test) % batch_size == 0 else len(cs_test) // batch_size + 1        \n",
    "                for i in tqdm(range(batch_num)):\n",
    "                    end = (i+1)*batch_size if (i+1)*batch_size <= len(cs_test) else len(cs_test)                \n",
    "                    cs_batch,es_batch = cs_test[i*batch_size :end], es_test[i*batch_size : end]\n",
    "                    score = model(cs_batch,es_batch) \n",
    "                    if args.weight_decay !=0.0:\n",
    "                        score = score[0]           \n",
    "                    y_pres.extend(score.view(-1).detach().cpu().numpy())  \n",
    "                    y_trues.extend(ls_test[i*batch_size : end])\n",
    "                    epitope.extend(es_test[i*batch_size : end])\n",
    "                    CDR3B.extend(cs_test[i*batch_size : end])\n",
    "                y_pres = np.array(y_pres)\n",
    "                y_pres = 1 / (1 + np.exp(-y_pres))\n",
    "                threshold = 0.5\n",
    "                y_preds_binary = [1 if p >= threshold else 0 for p in y_pres]\n",
    "                df = pd.DataFrame({'Epitope':epitope,'CDR3B': CDR3B, 'y_true': y_trues,'y_pred':y_preds_binary, 'y_prob': y_pres})\n",
    "                pd.DataFrame(df).to_csv(result_path+'probability.csv')   \n",
    "                test_auc = AUC(y_trues,y_pres)            \n",
    "                print('Epoch: ',e)\n",
    "                print('Test AUC: ',AUC(y_trues,y_pres))\n",
    "                record_aucs.append(AUC(y_trues,y_pres))\n",
    "        if e == args.step1 or e == args.step2:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr'] * 0.01\n",
    "            print('change the learning rate to 1e-4')\n",
    "        if e == args.step3:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr'] * 0.02\n",
    "            print('change')\n",
    "    if args.save_modle_path != 'None':\n",
    "        if not args.save_modle_path.endswith('.pth'):\n",
    "            args.save_modle_path = args.save_modle_path \n",
    "        torch.save(model.state_dict(),args.save_modle_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301263c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108fffbb",
   "metadata": {},
   "source": [
    "When retraining the model, you need to download the pre-trained `encoder_epi.pth` and `encoder_tcr.pth` from this link: https://github.com/jiangdada1221/TEINet/tree/master/encoders, and place them in the `encoders` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile_path =\"../data/train.csv\"\n",
    "testfile_path=\"../data/test.csv\"\n",
    "save_modle_path=\"../Retraining_model/Retraining_model.pth\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "Model_retraining(trainfile_path,testfile_path,save_modle_path,result_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc421e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8df642b2",
   "metadata": {},
   "source": [
    "# 3.Retraining_model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b17071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from scipy.special import expit\n",
    "import argparse\n",
    "def predict(cdrs,epitopes,labels,model,batch_size = 128):\n",
    "    batch_num_total = len(cdrs) // batch_size if len(cdrs) % batch_size == 0 else len(cdrs) // batch_size + 1\n",
    "    y_pres,y_trues ,epitope,y_preds_binary,CDR3B= [],[],[],[],[]\n",
    "    for batch_num in tqdm(range(batch_num_total)):\n",
    "        end = (batch_num+1)*batch_size if (batch_num+1)*batch_size <= len(cdrs) else len(cdrs)                \n",
    "        ts,es = cdrs[batch_num*batch_size:end],epitopes[batch_num * batch_size:end] \n",
    "        score = model(ts,es) \n",
    "        y_pres.extend(expit(score.view(-1).detach().cpu().numpy()))\n",
    "        y_trues.extend(labels[batch_num*batch_size : end])\n",
    "        epitope.extend(epitopes[batch_num*batch_size: end])\n",
    "        CDR3B.extend(cdrs[batch_num*batch_size: end])\n",
    "        threshold = 0.5\n",
    "        y_preds_binary = [1 if p >= threshold else 0 for p in y_pres]\n",
    "        df = pd.DataFrame({'epitope':epitope,'CDR3B': CDR3B, 'y_true': y_trues,'y_pred':y_preds_binary, 'y_prob': y_pres})\n",
    "        #name_str = '_'.join(name)\n",
    "        result_path1 = result_path +  'probability.csv'\n",
    "        pd.DataFrame(df).to_csv(result_path1)   \n",
    "    return {'epitope': epitope,'CDR3B': CDR3B,'y_true':y_trues,'y_prob':y_pres,'y_pred':y_preds_binary}\n",
    "\n",
    "class Args:\n",
    "    use_column='CDR3B'\n",
    "    batch_size=128\n",
    "  #  modelfile_path=modelfile_path\n",
    "    device='cuda:0'\n",
    "    def __init__(self, testfile_path, result_path,modelfile_path):\n",
    "        self.testfile_path = testfile_path \n",
    "        self.result_path = result_path\n",
    "        self.modelfile_path = modelfile_path\n",
    "        \n",
    "def Retraining_model_prediction(testfile_path,modelfile_path,result_path):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, matthews_corrcoef, precision_score,auc,roc_curve\n",
    "    args = Args(testfile_path=testfile_path, result_path=result_path, modelfile_path=modelfile_path)\n",
    "    f = pd.read_csv(args.testfile_path)\n",
    "    #load model\n",
    "    model_tcr = TCRpeg(hidden_size=768,num_layers = 3,load_data=False,embedding_path='encoders/aa_emb_tcr.txt',device=args.device)\n",
    "    model_tcr.create_model()\n",
    "    model_epi = TCRpeg(hidden_size=768,num_layers = 3,load_data=False,embedding_path='encoders/aa_emb_tcr.txt',device=args.device)\n",
    "    model_epi.create_model()\n",
    "    model = TEINet(en_tcr=model_tcr,en_epi = model_epi,cat_size=768*2,dropout=0.1,normalize=True,weight_decay = 0,device=args.device).to(args.device)\n",
    "    model.load_state_dict(torch.load(args.modelfile_path))\n",
    "    # model = torch.load(args.modelfile_path,map_location='cuda:0')\n",
    "    cdrs,epitopes,labels = f['CDR3B'].values, f['Epitope'].values, f['Affinity'].values\n",
    "    pres = predict(cdrs,epitopes,labels,model)\n",
    "    result = pd.DataFrame(pres)\n",
    "    result.to_csv(result_path+'probability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile_path=\"../data/Validation.csv\"\n",
    "modelfile_path=\"../Retraining_model/Retraining_model.pth\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "Retraining_model_prediction(testfile_path,modelfile_path,result_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEINet",
   "language": "python",
   "name": "teinet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
