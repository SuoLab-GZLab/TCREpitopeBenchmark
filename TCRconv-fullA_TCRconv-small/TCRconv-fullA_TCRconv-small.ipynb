{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa135910-1cae-4e8f-a086-e7aaa1a5bf9b",
   "metadata": {},
   "source": [
    "# 1.Original model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc5406-a454-454d-bc85-157bf8022876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from predictor import utils\n",
    "from pandas import read_csv, DataFrame\n",
    "from preprocessing.prep import get_protseqs_ntseqs,determine_tcr_seq_nt,determine_tcr_seq_vj\n",
    "from LM.bert_mdl import retrieve_model, compute_embs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def Original_model_prediction(dataPath='../data/pre_TCRconv_small.csv', mode= 'prediction',\\\n",
    "               epitope_labels= './data/unique_epitopes_test.npy', \\\n",
    "               chains='B', h_cdr31='CDR3B', h_long1='LongB', model_file='./model/statedict_vdjdb-b-small.pt',  \\\n",
    "               embedfile1= 'embeddings/bert_vdjdb-b-example_0.bin',\\\n",
    "               predfile='outputs/preds-b-example.csv', batch_size=256):\n",
    "\n",
    "    p = {      \n",
    "        'dataset': dataPath,\n",
    "        'chains': chains,\n",
    "        'epitope_labels': epitope_labels,\n",
    "        'embedtype': 'cdr3+context',\n",
    "        'h_cdr31': h_cdr31,\n",
    "        'h_long1': h_long1,\n",
    "        'embedfile1': embedfile1,\n",
    "        'h_cdr32': 'None',\n",
    "        'h_long2': 'None',    \n",
    "        'embedfile2': 'None',\n",
    "        'delimiter': ',',\n",
    "        \n",
    "        'mode': mode, \n",
    "        'folds': 'None',\n",
    "        'fold_num': 0,  \n",
    "        'h_epitope': 'Epitope',\n",
    "\n",
    "        'batch_size': 512,\n",
    "        'betas': [0.9, 0.999],\n",
    "        'use_pos_weight': True,\n",
    "        'dropouts': [0.1, 0.1],\n",
    "        'iters_adam': 2500,\n",
    "        'lr_conv': 0.0002,\n",
    "        'lr_linear': 0.01,\n",
    "        'T_anneal': 3000,\n",
    "        'iters_swa': 500,\n",
    "        'anneal_strategy': 'cos',\n",
    "        'lr_swa': 0.0001,\n",
    "        'T_anneal_swa': 300,\n",
    "        \n",
    "        'resultfile': './outputs/results.tsv',\n",
    "        'print_every': 100,\n",
    "        'lossfile': './outputs/loss_train.tsv',\n",
    "        'lossfile_test': './outputs/loss_test.tsv',\n",
    "        # 'model_folder': model_folder,\n",
    "        'params_to_print': ['fold_num'],\n",
    "\n",
    "        'binary': False,\n",
    "        'binary_label': None,\n",
    "        \n",
    "        'append_oh': False,\n",
    "        'kernel_sizes': [5,9,15,21,3],\n",
    "        'pool': 'max',\n",
    "        'h_v1': 'none',\n",
    "        'h_j1': 'none',\n",
    "        'h_v2': 'none',\n",
    "        'h_j2': 'none',\n",
    "        'h_nt1': 'none',\n",
    "        'h_nt2': 'none',\n",
    "        'guess_allele01': True,\n",
    "        'model_file': model_file,  \n",
    "        'use_LM': False,\n",
    "        'num_features': 1024,\n",
    "        'input_type': 'tcr+cdr3',\n",
    "        'predfile': predfile,  \n",
    "        'additional_columns': [],\n",
    "        'decimals': 4,\n",
    "        \n",
    "        }\n",
    "\n",
    "    p['two_chains']=len(p['chains'])>1\n",
    "    p['table']=(p['params_to_print'],[p[h] for h in p['params_to_print']])\n",
    "    p['save_intermediate'] = p['lossfile'].lower()!='none' or p['lossfile_test'].lower()!='none'\n",
    "    \n",
    "    data = read_csv(p['dataset'],delimiter=p['delimiter'],dtype=str,keep_default_na=False)\n",
    "    epis_u = np.load(p['epitope_labels'])\n",
    "    n_labels=len(epis_u)\n",
    "    n_chains=len(p['chains'])\n",
    "    \n",
    "    resfile = p['predfile']\n",
    "    with open(resfile,'w') as f:\n",
    "        f.write(p['delimiter'].join(['TCR'+c for c in p['chains']]\n",
    "                +p['additional_columns']+list(epis_u))+'\\n')\n",
    "    \n",
    "    # Load TCRconv model\n",
    "    model = utils.load_model(p['model_file'],p,p['num_features'],n_labels,device)\n",
    "    # embeddings / embedding-models\n",
    "    if p['use_LM']:\n",
    "        LM=retrieve_model().to(device)\n",
    "    \n",
    "    # Get requested sequences for genes\n",
    "    geneseqs={}\n",
    "    for ic,chain in enumerate(p['chains']):\n",
    "        c=str(ic+1)\n",
    "        if p['input_type']=='cdr3+nt':\n",
    "            geneseqs['protV'+c],geneseqs['protJ'+c],geneseqs['ntV'+c],geneseqs['ntJ'+c] = \\\n",
    "                    get_protseqs_ntseqs(chain=chain)\n",
    "        elif p['input_type']=='cdr3+vj':\n",
    "            geneseqs['protV'+c],geneseqs['protJ'+c],_,_ = get_protseqs_ntseqs(chain=chain)\n",
    "    \n",
    "    I=[]\n",
    "    ts_all = [[] for c in p['chains']] # separate list for each chain\n",
    "    icount,i0 = 0,0\n",
    "    imax=len(data)-1\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "    \n",
    "        if p['input_type']=='cdr3+nt':\n",
    "            tcr12 = []\n",
    "            for ic in range(n_chains):\n",
    "                c=str(ic+1)\n",
    "                t,_,_ = determine_tcr_seq_nt(data[p['h_nt'+c]][i],data[p['h_cdr3'+c]][i],geneseqs['protV'+c],\n",
    "                        geneseqs['protJ'+c],geneseqs['ntV'+c],geneseqs['ntJ'+c],guess01=p['guess_allele01'])\n",
    "                tcr12.append(t)\n",
    "        elif p['input_type']=='cdr3+vj':\n",
    "            tcr12 = []\n",
    "            for ic in range(n_chains):\n",
    "                c=str(ic+1)\n",
    "                t = determine_tcr_seq_vj(data[p['h_cdr3'+c]][i],data[p['h_v'+c]][i],data[p['h_j'+c]][i],\n",
    "                    geneseqs['protV'+c],geneseqs['protJ'+c],guess01=p['guess_allele01'])\n",
    "                tcr12.append(t)\n",
    "        elif p['input_type']=='cdr3':\n",
    "            tcr12 = [data[p['h_cdr3'+str(ic+1)]][i] for ic in range(n_chains)]\n",
    "        else: # tcr+cdr3 / tcr\n",
    "            tcr12 = [data[p['h_long'+str(ic+1)]][i] for ic in range(n_chains)]\n",
    "    \n",
    "    \n",
    "        # Check if (either) sequence is empty\n",
    "        if np.any([t=='' for t in tcr12]):\n",
    "            I.append(False)\n",
    "            for ic in range(n_chains):\n",
    "                ts_all[ic].append(tcr12[ic])\n",
    "        else:\n",
    "            I.append(True)\n",
    "            for ic in range(n_chains):\n",
    "                ts_all[ic].append(tcr12[ic])\n",
    "            icount+=1\n",
    "    \n",
    "    \n",
    "        if icount==p['batch_size'] or i==imax:\n",
    "            ts_all =[np.array(t) for t in ts_all]\n",
    "            if icount>0:\n",
    "                I= np.array(I,dtype=bool)\n",
    "                if p['use_LM']: # If LM is used, compute embeddings\n",
    "                    cdr3s = data[p['h_cdr31']][i0:i+1][I].values\n",
    "                    print('computing embeddings: {:d}-{:d}/{:d}'.format(i0,i,imax))\n",
    "                    if p['embedtype']=='cdr3+context':\n",
    "                        embeddings1 = compute_embs(LM, ts_all[0][I], cdr3s)\n",
    "                        embeddings1 = utils.stack_embeddings(embeddings1,device,p['append_oh'],cdr3s)\n",
    "                    else:\n",
    "                        embeddings1 = compute_embs(LM, ts_all[0][I], None)\n",
    "                        embeddings1 = utils.stack_embeddings(embeddings1,device,p['append_oh'])\n",
    "    \n",
    "                    if p['two_chains']:\n",
    "                        cdr3s = data[p['h_cdr32']][i0:i+1][I].values\n",
    "                        if p['embedtype']=='cdr3+context':\n",
    "                            embeddings2 = compute_embs(LM,ts_all[1][I], cdr3s)\n",
    "                            embeddings2 = utils.stack_embeddings(embeddings2,device,p['append_oh'],cdr3s)\n",
    "                        else:\n",
    "                            embeddings2 = compute_embs(LM, ts_all[1][I], None)\n",
    "                            embeddings2 = utils.stack_embeddings(embeddings2,device,p['append_oh'])\n",
    "    \n",
    "                else: # 1-2 embedding dictionaries are used\n",
    "    \n",
    "                    #print(ts_all[0][I])\n",
    "                    cdr3s = data[p['h_cdr31']][i0:i+1][I].values\n",
    "                    cdr3max = utils.maxlen(cdr3s)\n",
    "                    embeddings1 = utils.get_embeddings(utils.get_embedding_dict(p['embedfile1']),\n",
    "                                cdr3s, ts_all[0][I], cdr3max, device, p['append_oh'])\n",
    "                    if p['two_chains']:\n",
    "                        cdr3s = data[p['h_cdr32']][i0:i+1][I].values\n",
    "                        cdr3max = utils.maxlen(cdr3s)\n",
    "                        embeddings2 = utils.get_embeddings(utils.get_embedding_dict(p['embedfile2']),\n",
    "                                cdr3s,ts_all[1][I], cdr3max, device, p['append_oh'])\n",
    "    \n",
    "                # Predictions\n",
    "                if p['two_chains']:\n",
    "                    output = model(embeddings1,embeddings2).detach().cpu().numpy()\n",
    "                else:\n",
    "                    output = model(embeddings1).detach().cpu().numpy()\n",
    "                output = utils.toprob(output)\n",
    "    \n",
    "                pred_ar = np.ones((len(I),n_labels),dtype=float)*np.nan\n",
    "                pred_ar[I,:]= output\n",
    "    \n",
    "            else: # No proper sequences were found, add fillers\n",
    "                pred_ar = np.ones((len(I),n_labels),dtype=float)*np.nan\n",
    "    \n",
    "            # append results to result file\n",
    "            df = DataFrame(np.concatenate([np.expand_dims(ts_all[i],1) for i in range(len(ts_all))] \\\n",
    "                +[np.expand_dims(data[col].values[i0:i+1],1) for col in p['additional_columns']] \\\n",
    "                +[np.round(pred_ar,p['decimals'])],axis=1))\n",
    "            df.to_csv(resfile,sep=p['delimiter'],mode='a',header=False,index=False)\n",
    "    \n",
    "            I = []\n",
    "            ts_all = [[] for c in p['chains']]\n",
    "            icount = 0\n",
    "            i0 = i+1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ee1ed-7a74-482a-974a-1af8f2f73ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def fix(data_ori,data_new):\n",
    "    data=pd.read_csv(data_ori)\n",
    "    #'CDR3.beta', 'antigen_epitope','mhc.a','label','negative.source','license'\n",
    "    data.rename(columns={'CDR3B':'CDR3.beta','Epitope':'antigen_epitope','MHC':'mhc.a','Affinity':'label'},inplace=True)\n",
    "    df=data[['CDR3.beta', 'antigen_epitope','mhc.a','label']]\n",
    "    \n",
    "    df_epi = pd.DataFrame([peptides.Peptide(s).descriptors() for s in df.antigen_epitope])\n",
    "    df_epi.columns='epitope_'+df_epi.columns\n",
    "    df_cdrb = pd.DataFrame([peptides.Peptide(s).descriptors() for s in df['CDR3.beta']])\n",
    "    df_cdrb.columns='cdr3_'+df_cdrb.columns\n",
    "    df=pd.concat([df, df_cdrb, df_epi],axis=1)\n",
    "    df.to_csv(data_new)\n",
    "        \n",
    "preData_ori=\"../data/test_CDR3B_others.csv\"\n",
    "preData=f\"../data/pre_TCRconv_small.csv\"\n",
    "fix(preData_ori,preData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38afc3-c335-4f96-a05e-c2051dab91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from preprocessing import prep \n",
    "filename = '../data/pre_TCRconv_small.csv'\n",
    "epis = np.loadtxt(filename,usecols=(0),unpack=True,delimiter=',',skiprows=1,comments=None,dtype='str')\n",
    "epis_u,labels = prep.get_labels(epis)\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "np.save('./data/unique_epitopes_test.npy',epis_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e067-98a5-48e9-831c-8cccf74f86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_model_prediction(dataPath='../data/pre_TCRconv_small.csv', mode= 'prediction',\\\n",
    "               epitope_labels= './data/unique_epitopes_test.npy', \\\n",
    "               chains='B', h_cdr31='CDR3B', h_long1='LongB', model_file='../Original_model/TCRconv_small.pt', \\\n",
    "               embedfile1= 'embeddings/bert_vdjdb-b-small.bin',\\\n",
    "               predfile=\"../result_path/Original_model_prediction/test.csv', batch_size=256\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff9a80-5b91-45ca-a15e-3bf34f70c072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963d390b-fc0b-492f-91b0-34602fe5dc1d",
   "metadata": {},
   "source": [
    "# 2.Model retraining: fullA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c9ce3-e9ed-4778-8e6d-9afb26ccecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('./tcrconv/')\n",
    "from os import path, mkdir\n",
    "\n",
    "sys.path.append('./LM/')\n",
    "import pandas as pd\n",
    "from preprocessing import prep\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from predictor import utils\n",
    "from pandas import read_csv, DataFrame\n",
    "from preprocessing.prep import get_protseqs_ntseqs,determine_tcr_seq_nt,determine_tcr_seq_vj\n",
    "from LM.bert_mdl import retrieve_model, compute_embs\n",
    "from argparse import ArgumentParser,ArgumentDefaultsHelpFormatter,ArgumentTypeError\n",
    "from bert_mdl import retrieve_model, extract_and_save_embeddings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def fix(data_ori,data_new):\n",
    "    data=pd.read_csv(data_ori)\n",
    "    #'CDR3.beta', 'antigen_epitope','mhc.a','label','negative.source','license'\n",
    "    # data.drop(columns=['VB','JB'],inplace=True)\n",
    "    data.rename(columns={'TRBV':'VB','TRBJ':'JB','TRAV':'VA','TRAJ':'JA','Affinity':'label'},inplace=True)  \n",
    "    data=data.loc[data.label==1]\n",
    "    data['Subject']='PMID:0'\n",
    "    df=data[['Epitope', 'Subject', 'CDR3A', 'VA', 'JA', 'LongA']]\n",
    "    df.to_csv(data_new,index=False)\n",
    "        \n",
    "def extract_epitopes(test_file,epitope_file):\n",
    "    epis = np.loadtxt(test_file,usecols=(0),unpack=True,delimiter=',',skiprows=1,comments=None,dtype='str')\n",
    "    epis_u,labels = prep.get_labels(epis)\n",
    "    np.save(epitope_file,epis_u)\n",
    "\n",
    "\n",
    "def make_embeddings(testfile, embed_model):\n",
    "    print(\"start embedding\")\n",
    "    model = retrieve_model().to(device)\n",
    "    # embed_model='embeddings/bert_vdjdb-b-example'#name for the model. Will be used if the model or results are saved\n",
    "    # testfile='./data/vdjdb-b-example.csv' #filename of the used dataset\n",
    "    delimiter=',' #Column delimiter in dataset file\n",
    "    h_cdr3='CDR3A'#Column name for CDR3 of chain 1 in dataset file\n",
    "    h_long='LongA'#Column name for Long TCR-sequence of chain 1 in dataset file\n",
    "    seqs_per_file=50000#Maximum number of sequences in one embedding file. If there are more sequences, the embeddings will be split into several files.\n",
    "    \n",
    "    # extract and save some embeddings\n",
    "    extract_and_save_embeddings(model, data_f_n=testfile, sequence_col=h_long, cdr3_col=h_cdr3, seqs_per_file=seqs_per_file, emb_name=embed_model,separator=delimiter)\n",
    "    print(\"finished embedding\")\n",
    "\n",
    "def train_and_save(modelName='vdjdb-b-example', dataPath=\"./data/vdjdb-b-example.csv\", \\\n",
    "               epitope_labels= './data/unique_epitopes_vdjdb-b-example.npy', mode= 'train', \\\n",
    "               chains='A', h_cdr32='CDR3A', h_long2='LongA', \\\n",
    "               embedfile2= 'embeddings/bert_vdjdb-b-example_0.bin',\\\n",
    "               model_folder='models_retrained', save_model_path='models_retrained/model_example.pt'):\n",
    "    p = {\n",
    "        \n",
    "        'dataset': dataPath,\n",
    "        'chains': chains,\n",
    "        'epitope_labels': epitope_labels,\n",
    "        'embedtype': 'cdr3+context',\n",
    "        'h_cdr31': h_cdr32,\n",
    "        'h_long1': h_long2,\n",
    "        'embedfile1': embedfile2,\n",
    "        'h_cdr32': 'None',\n",
    "        'h_long2': 'None',    \n",
    "        'embedfile2': 'None',\n",
    "        'delimiter': ',',\n",
    "        \n",
    "        'mode': mode, \n",
    "        'name': modelName,\n",
    "        'folds': 'None',\n",
    "        'fold_num': 0,  \n",
    "        'h_epitope': 'Epitope',\n",
    "\n",
    "        'batch_size': 512,\n",
    "        'betas': [0.9, 0.999],\n",
    "        'use_pos_weight': True,\n",
    "        'dropouts': [0.1, 0.1],\n",
    "        'iters_adam': 2500,\n",
    "        'lr_conv': 0.0002,\n",
    "        'lr_linear': 0.01,\n",
    "        'T_anneal': 3000,\n",
    "        'iters_swa': 500,\n",
    "        'anneal_strategy': 'cos',\n",
    "        'lr_swa': 0.0001,\n",
    "        'T_anneal_swa': 300,\n",
    "        \n",
    "        'resultfile': None,\n",
    "        'print_every': 100,\n",
    "        'lossfile': 'None',#'./outputs/loss_train.tsv',\n",
    "        'lossfile_test': 'None',\n",
    "        'model_folder': model_folder,\n",
    "        'params_to_print': ['name', 'fold_num'],\n",
    "\n",
    "        'binary': False,\n",
    "        'binary_label': None,\n",
    "        \n",
    "        'append_oh': False,\n",
    "        'kernel_sizes': [5,9,15,21,3],\n",
    "        'pool': 'max',\n",
    "            \n",
    "        \n",
    "    }\n",
    "    \n",
    "    p['two_chains']=len(p['chains'])>1\n",
    "    p['table']=(p['params_to_print'],[p[h] for h in p['params_to_print']])\n",
    "    p['save_intermediate'] = p['lossfile'].lower()!='none' or p['lossfile_test'].lower()!='none'\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # print(f\"Using device: {device}\")\n",
    "    \n",
    "    mode =p['mode'].lower()\n",
    "    # Get dataloaders\n",
    "    if mode=='cv':\n",
    "        usetypes=['train','test']\n",
    "    elif mode=='train':\n",
    "        usetypes=['train']\n",
    "        \n",
    "    print('loading data...')\n",
    "    loader, n_categories, n_feat = utils.get_dataloaders(p,device=device,usetypes=usetypes)\n",
    "    # Create model\n",
    "    print('creating model...')\n",
    "    model = utils.construct_model(p,n_feat,n_categories['train'],device)\n",
    "    # Create requested loss and result files if they don't exist yet\n",
    "    # utils.create_resultfiles(p,n_categories)\n",
    "    # Model training\n",
    "    print('training model...')\n",
    "    model = utils.iterate_model_batches_swa(model,loader,p,device)\n",
    "    print('saving model...')\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    # Save model if model_folder is given\n",
    "    # if p['model_folder'] != 'None':\n",
    "    #     if not path.isdir(p['model_folder']):\n",
    "    #         mkdir(p['model_folder'])\n",
    "    #     modelfile = 'statedict_'+p['name'] + ('_'+str(p['fold_num']))*(p['mode']=='cv') + '.pt'\n",
    "    #     torch.save(model.state_dict(), p['model_folder']+'/'+modelfile)\n",
    "    \n",
    "    # # Make predictions and save results\n",
    "    # if p['resultfile'] != 'None':\n",
    "    #     y_score, labels = utils.get_yscore(model,loader['test'],useAB=p['two_chains'])\n",
    "    #     y_score=y_score[:,:n_categories['test']]\n",
    "    #     utils.save_results(y_score,labels,p,n_categories['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ae427-2643-4270-93ab-61d906e970ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile_path =\"../data/train_CDR3B_others.csv\"\n",
    "trainfile_out==\"../data/TCRconv-A/\"\n",
    "os.makedirs(trainfile_out, exist_ok=True)\n",
    "\n",
    "trainfile_new=trainfile_out+\"train_CDR3B_others.csv\"\n",
    "fix(trainfile_path,trainfile_new)\n",
    "epitope_file=trainfile_out+'train_unique_epitopes.npy'\n",
    "extract_epitopes(trainfile_new,epitope_file)\n",
    "\n",
    "embed_model=f'./embeddings/train/TCRconv_fullA/bert'+\n",
    "os.makedirs(f'./embeddings/train/TCRconv_fullA/', exist_ok=True)\n",
    "make_embeddings(trainfile_new, embed_model)\n",
    "save_dir=\"../result_path/Retraining_model_prediction\"\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "save_model_path= \"../Retraining_model/Retraining_model.pt\"\n",
    "Model_retraining(dataPath=trainfile_new, \\\n",
    "               epitope_labels= epitope_file, mode= 'train', \\\n",
    "               chains='A', h_cdr32='CDR3A', h_long2='LongA', \\\n",
    "               embedfile2= embed_model+'_0.bin',\\\n",
    "               save_model_path=save_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cd264-a43d-42f4-86c8-495bf92baa59",
   "metadata": {},
   "source": [
    "# 3.Retraining_model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805b9f3-2f47-4730-b4ec-3dfb41883ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./LM/')\n",
    "import pandas as pd\n",
    "from preprocessing import prep\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from predictor import utils\n",
    "from pandas import read_csv, DataFrame\n",
    "from preprocessing.prep import get_protseqs_ntseqs,determine_tcr_seq_nt,determine_tcr_seq_vj\n",
    "from LM.bert_mdl import retrieve_model, compute_embs\n",
    "from argparse import ArgumentParser,ArgumentDefaultsHelpFormatter,ArgumentTypeError\n",
    "from bert_mdl import retrieve_model, extract_and_save_embeddings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def fix(data_ori,data_new):\n",
    "    data=pd.read_csv(data_ori)\n",
    "    #'CDR3.beta', 'antigen_epitope','mhc.a','label','negative.source','license'\n",
    "    data.drop(columns=['VB','JB'],inplace=True)\n",
    "    data.rename(columns={'TRBV':'VB','TRBJ':'JB','TRAV':'VA','TRAJ':'JA','Affinity':'label'},inplace=True)\n",
    "    \n",
    "    data['Subject']='PMID:0'\n",
    "    df=data[['Epitope', 'Subject', 'CDR3B', 'VB', 'JB', 'CDR3A', 'VA', 'JA', 'LongB','LongA']]\n",
    "    df.to_csv(data_new,index=False)\n",
    "        \n",
    "def extract_epitopes(test_file,epitope_file):\n",
    "    epis = np.loadtxt(test_file,usecols=(0),unpack=True,delimiter=',',skiprows=1,comments=None,dtype='str')\n",
    "    epis_u,labels = prep.get_labels(epis)\n",
    "    np.save(epitope_file,epis_u)\n",
    "\n",
    "\n",
    "def make_embeddings(testfile, embed_model):\n",
    "    print(\"start embedding\")\n",
    "    model = retrieve_model().to(device)\n",
    "    # embed_model='embeddings/bert_vdjdb-b-example'#name for the model. Will be used if the model or results are saved\n",
    "    # testfile='./data/vdjdb-b-example.csv' #filename of the used dataset\n",
    "    delimiter=',' #Column delimiter in dataset file\n",
    "    h_cdr3='CDR3B'#Column name for CDR3 of chain 1 in dataset file\n",
    "    h_long='LongB'#Column name for Long TCR-sequence of chain 1 in dataset file\n",
    "    seqs_per_file=50000#Maximum number of sequences in one embedding file. If there are more sequences, the embeddings will be split into several files.\n",
    "    \n",
    "    # extract and save some embeddings\n",
    "    extract_and_save_embeddings(model, data_f_n=testfile, sequence_col=h_long, cdr3_col=h_cdr3, seqs_per_file=seqs_per_file, emb_name=embed_model,separator=delimiter)\n",
    "    print(\"finished embedding\")\n",
    "\n",
    "def make_embeddings_a(testfile, embed_model):\n",
    "    print(\"start embedding\")\n",
    "    model = retrieve_model().to(device)\n",
    "    # embed_model='embeddings/bert_vdjdb-b-example'#name for the model. Will be used if the model or results are saved\n",
    "    # testfile='./data/vdjdb-b-example.csv' #filename of the used dataset\n",
    "    delimiter=',' #Column delimiter in dataset file\n",
    "    h_cdr3='CDR3A'#Column name for CDR3 of chain 1 in dataset file\n",
    "    h_long='LongA'#Column name for Long TCR-sequence of chain 1 in dataset file\n",
    "    seqs_per_file=50000#Maximum number of sequences in one embedding file. If there are more sequences, the embeddings will be split into several files.\n",
    "    \n",
    "    # extract and save some embeddings\n",
    "    extract_and_save_embeddings(model, data_f_n=testfile, sequence_col=h_long, cdr3_col=h_cdr3, seqs_per_file=seqs_per_file, emb_name=embed_model,separator=delimiter)\n",
    "    print(\"finished embedding\")\n",
    "    \n",
    "def predict_and_save(dataPath='./data/vdjdb-b-example.csv', mode= 'prediction',\\\n",
    "               epitope_labels= './data/unique_epitopes_vdjdb-b-example.npy', \\\n",
    "               chains='AB', h_cdr31='CDR3B', h_long1='LongB',  h_cdr32='CDR3A', h_long2='LongA',  h_v1='VB', h_j1='JB',h_v2='VA', h_j2='JA', model_file='./model/statedict_vdjdb-b-small.pt',  \\\n",
    "               embedfile1= 'embeddings/bert_vdjdb-b-example_0.bin',  embedfile2= 'embeddings/bert_vdjdb-b-example_0.bin',\\\n",
    "               predfile='outputs/preds-b-example.csv', result_path='outputs/b-example_probability.csv', batch_size=256,testfile=None):\n",
    "\n",
    "    p = {      \n",
    "        'dataset': dataPath,\n",
    "        'chains': chains,\n",
    "        'epitope_labels': epitope_labels,\n",
    "        'embedtype': 'cdr3+context',\n",
    "        'h_cdr31': h_cdr31,\n",
    "        'h_long1': h_long1,\n",
    "        'embedfile1': embedfile1,\n",
    "        'h_cdr32':h_cdr32,\n",
    "        'h_long2':h_long2,    \n",
    "        'embedfile2': embedfile2,\n",
    "        'delimiter': ',',\n",
    "        \n",
    "        'mode': mode, \n",
    "        # 'name': modelName,\n",
    "        'folds': 'None',\n",
    "        'fold_num': 0,  \n",
    "        'h_epitope': 'Epitope',\n",
    "\n",
    "        'batch_size': 512,\n",
    "        'betas': [0.9, 0.999],\n",
    "        'use_pos_weight': True,\n",
    "        'dropouts': [0.1, 0.1],\n",
    "        'iters_adam': 2500,\n",
    "        'lr_conv': 0.0002,\n",
    "        'lr_linear': 0.01,\n",
    "        'T_anneal': 3000,\n",
    "        'iters_swa': 500,\n",
    "        'anneal_strategy': 'cos',\n",
    "        'lr_swa': 0.0001,\n",
    "        'T_anneal_swa': 300,\n",
    "        \n",
    "        'resultfile': './outputs/results.tsv',\n",
    "        'print_every': 100,\n",
    "        'lossfile': './outputs/loss_train.tsv',\n",
    "        'lossfile_test': './outputs/loss_test.tsv',\n",
    "        # 'model_folder': model_folder,\n",
    "        'params_to_print': ['fold_num'],\n",
    "\n",
    "        'binary': False,\n",
    "        'binary_label': None,\n",
    "        \n",
    "        'append_oh': False,\n",
    "        'kernel_sizes': [5,9,15,21,3],\n",
    "        'pool': 'max',\n",
    "        'h_v1': h_v1,\n",
    "        'h_j1': h_j1,\n",
    "        'h_v2': h_v2,\n",
    "        'h_j2': h_j2,\n",
    "        'h_nt1': 'none',\n",
    "        'h_nt2': 'none',\n",
    "        'guess_allele01': True,\n",
    "        'model_file': model_file,  \n",
    "        'use_LM': False,\n",
    "        'num_features': 1024,\n",
    "        'input_type': 'tcr+cdr3',\n",
    "        'predfile': predfile,  \n",
    "        'additional_columns': [],\n",
    "        'decimals': 4,\n",
    "        \n",
    "        }\n",
    "\n",
    "    p['two_chains']=len(p['chains'])>1\n",
    "    p['table']=(p['params_to_print'],[p[h] for h in p['params_to_print']])\n",
    "    p['save_intermediate'] = p['lossfile'].lower()!='none' or p['lossfile_test'].lower()!='none'\n",
    "    \n",
    "    data = read_csv(p['dataset'],delimiter=p['delimiter'],dtype=str,keep_default_na=False)\n",
    "    epis_u = np.load(p['epitope_labels'])\n",
    "    n_labels=len(epis_u)\n",
    "    n_chains=len(p['chains'])\n",
    "    \n",
    "    resfile = p['predfile']\n",
    "    with open(resfile,'w') as f:\n",
    "        f.write(p['delimiter'].join(['TCR'+c for c in p['chains']]\n",
    "                +p['additional_columns']+list(epis_u))+'\\n')\n",
    "    \n",
    "    # Load TCRconv model\n",
    "    model = utils.load_model(p['model_file'],p,p['num_features'],n_labels,device)\n",
    "    # embeddings / embedding-models\n",
    "    if p['use_LM']:\n",
    "        LM=retrieve_model().to(device)\n",
    "    \n",
    "    # Get requested sequences for genes\n",
    "    geneseqs={}\n",
    "    for ic,chain in enumerate(p['chains']):\n",
    "        c=str(ic+1)\n",
    "        if p['input_type']=='cdr3+nt':\n",
    "            geneseqs['protV'+c],geneseqs['protJ'+c],geneseqs['ntV'+c],geneseqs['ntJ'+c] = \\\n",
    "                    get_protseqs_ntseqs(chain=chain)\n",
    "        elif p['input_type']=='cdr3+vj':\n",
    "            geneseqs['protV'+c],geneseqs['protJ'+c],_,_ = get_protseqs_ntseqs(chain=chain)\n",
    "    \n",
    "    I=[]\n",
    "    ts_all = [[] for c in p['chains']] # separate list for each chain\n",
    "    icount,i0 = 0,0\n",
    "    imax=len(data)-1\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "    \n",
    "        if p['input_type']=='cdr3+nt':\n",
    "            tcr12 = []\n",
    "            for ic in range(n_chains):\n",
    "                c=str(ic+1)\n",
    "                t,_,_ = determine_tcr_seq_nt(data[p['h_nt'+c]][i],data[p['h_cdr3'+c]][i],geneseqs['protV'+c],\n",
    "                        geneseqs['protJ'+c],geneseqs['ntV'+c],geneseqs['ntJ'+c],guess01=p['guess_allele01'])\n",
    "                tcr12.append(t)\n",
    "        elif p['input_type']=='cdr3+vj':\n",
    "            tcr12 = []\n",
    "            for ic in range(n_chains):\n",
    "                c=str(ic+1)\n",
    "                t = determine_tcr_seq_vj(data[p['h_cdr3'+c]][i],data[p['h_v'+c]][i],data[p['h_j'+c]][i],\n",
    "                    geneseqs['protV'+c],geneseqs['protJ'+c],guess01=p['guess_allele01'])\n",
    "                tcr12.append(t)\n",
    "        elif p['input_type']=='cdr3':\n",
    "            tcr12 = [data[p['h_cdr3'+str(ic+1)]][i] for ic in range(n_chains)]\n",
    "        else: # tcr+cdr3 / tcr\n",
    "            tcr12 = [data[p['h_long'+str(ic+1)]][i] for ic in range(n_chains)]\n",
    "    \n",
    "    \n",
    "        # Check if (either) sequence is empty\n",
    "        if np.any([t=='' for t in tcr12]):\n",
    "            I.append(False)\n",
    "            for ic in range(n_chains):\n",
    "                ts_all[ic].append(tcr12[ic])\n",
    "        else:\n",
    "            I.append(True)\n",
    "            for ic in range(n_chains):\n",
    "                ts_all[ic].append(tcr12[ic])\n",
    "            icount+=1\n",
    "    \n",
    "    \n",
    "        if icount==p['batch_size'] or i==imax:\n",
    "            ts_all =[np.array(t) for t in ts_all]\n",
    "            if icount>0:\n",
    "                I= np.array(I,dtype=bool)\n",
    "                if p['use_LM']: # If LM is used, compute embeddings\n",
    "                    cdr3s = data[p['h_cdr31']][i0:i+1][I].values\n",
    "                    print('computing embeddings: {:d}-{:d}/{:d}'.format(i0,i,imax))\n",
    "                    if p['embedtype']=='cdr3+context':\n",
    "                        embeddings1 = compute_embs(LM, ts_all[0][I], cdr3s)\n",
    "                        embeddings1 = utils.stack_embeddings(embeddings1,device,p['append_oh'],cdr3s)\n",
    "                    else:\n",
    "                        embeddings1 = compute_embs(LM, ts_all[0][I], None)\n",
    "                        embeddings1 = utils.stack_embeddings(embeddings1,device,p['append_oh'])\n",
    "    \n",
    "                    if p['two_chains']:\n",
    "                        cdr3s = data[p['h_cdr32']][i0:i+1][I].values\n",
    "                        if p['embedtype']=='cdr3+context':\n",
    "                            embeddings2 = compute_embs(LM,ts_all[1][I], cdr3s)\n",
    "                            embeddings2 = utils.stack_embeddings(embeddings2,device,p['append_oh'],cdr3s)\n",
    "                        else:\n",
    "                            embeddings2 = compute_embs(LM, ts_all[1][I], None)\n",
    "                            embeddings2 = utils.stack_embeddings(embeddings2,device,p['append_oh'])\n",
    "    \n",
    "                else: # 1-2 embedding dictionaries are used\n",
    "    \n",
    "                    #print(ts_all[0][I])\n",
    "                    cdr3s = data[p['h_cdr31']][i0:i+1][I].values\n",
    "                    cdr3max = utils.maxlen(cdr3s)\n",
    "                    embeddings1 = utils.get_embeddings(utils.get_embedding_dict(p['embedfile1']),\n",
    "                                cdr3s, ts_all[0][I], cdr3max, device, p['append_oh'])\n",
    "                    if p['two_chains']:\n",
    "                        cdr3s = data[p['h_cdr32']][i0:i+1][I].values\n",
    "                        cdr3max = utils.maxlen(cdr3s)\n",
    "                        embeddings2 = utils.get_embeddings(utils.get_embedding_dict(p['embedfile2']),\n",
    "                                cdr3s,ts_all[1][I], cdr3max, device, p['append_oh'])\n",
    "    \n",
    "                # Predictions\n",
    "                if p['two_chains']:\n",
    "                    output = model(embeddings1,embeddings2).detach().cpu().numpy()\n",
    "                else:\n",
    "                    output = model(embeddings1).detach().cpu().numpy()\n",
    "                output = utils.toprob(output)\n",
    "    \n",
    "                pred_ar = np.ones((len(I),n_labels),dtype=float)*np.nan\n",
    "                pred_ar[I,:]= output\n",
    "    \n",
    "            else: # No proper sequences were found, add fillers\n",
    "                pred_ar = np.ones((len(I),n_labels),dtype=float)*np.nan\n",
    "    \n",
    "            # append results to result file\n",
    "            df = DataFrame(np.concatenate([np.expand_dims(ts_all[i],1) for i in range(len(ts_all))] \\\n",
    "                +[np.expand_dims(data[col].values[i0:i+1],1) for col in p['additional_columns']] \\\n",
    "                +[np.round(pred_ar,p['decimals'])],axis=1))\n",
    "            df.to_csv(resfile,sep=p['delimiter'],mode='a',header=False,index=False)\n",
    "           \n",
    "            I = []\n",
    "            ts_all = [[] for c in p['chains']]\n",
    "            icount = 0\n",
    "            i0 = i+1  \n",
    "            \n",
    "    #process result file\n",
    "    df_result=pd.read_csv(resfile)\n",
    "    probability = df_result[['TCRB']]\n",
    "    \n",
    "    df_ori=pd.read_csv(dataPath)\n",
    "    print('df_ori',df_ori)\n",
    "    for k in range(len(df_ori)):\n",
    "        probability.loc[probability.TCRB==df_ori['LongA'][k],'Epitope']=df_ori['Epitope'][k]\n",
    "    print(probability)\n",
    "    get_label=df_result[df_result.columns[1:]]\n",
    "    get_label = get_label.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    pre_epi_ls=get_label.idxmax(axis=1).tolist()   \n",
    "    probability['Epitope_pred']=pre_epi_ls\n",
    "    \n",
    "\n",
    "    get_label['y_prob']=get_label.max(axis=1)\n",
    "    probability['y_prob']=get_label['y_prob'].tolist()\n",
    "    probability['y_pred']=1\n",
    "    for k in range(len(probability)):\n",
    "        if probability['Epitope_pred'][k]!=probability['Epitope'][k]:\n",
    "            probability['y_prob'][k]=0\n",
    "            probability['y_pred'][k]=0\n",
    "\n",
    "\n",
    "    df_tmp=pd.read_csv(testfile)\n",
    "    for i in range(len(df_tmp)):\n",
    "        probability.loc[probability.TCRB==df_tmp['LongA'][i],'y_true']=df_tmp['Affinity'][i]\n",
    "        \n",
    "    print(probability)\n",
    "    probability.to_csv(result_path+'probability.csv',index=False)\n",
    "    print('done saving!')\n",
    "    sys.path.append('./LM/')\n",
    "import pandas as pd\n",
    "from preprocessing import prep\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from predictor import utils\n",
    "from pandas import read_csv, DataFrame\n",
    "from preprocessing.prep import get_protseqs_ntseqs,determine_tcr_seq_nt,determine_tcr_seq_vj\n",
    "from LM.bert_mdl import retrieve_model, compute_embs\n",
    "from argparse import ArgumentParser,ArgumentDefaultsHelpFormatter,ArgumentTypeError\n",
    "from bert_mdl import retrieve_model, extract_and_save_embeddings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def fix(data_ori,data_new):\n",
    "    data=pd.read_csv(data_ori)\n",
    "    #'CDR3.beta', 'antigen_epitope','mhc.a','label','negative.source','license'\n",
    "    data.rename(columns={'TRBV':'VB','TRBJ':'JB','TRAV':'VA','TRAJ':'JA','Affinity':'label'},inplace=True)\n",
    "    \n",
    "    data['Subject']='PMID:0'\n",
    "    df=data[['Epitope', 'Subject', 'CDR3B', 'VB', 'JB', 'CDR3A', 'VA', 'JA', 'LongB','LongA']]\n",
    "    df.to_csv(data_new,index=False)\n",
    "        \n",
    "def extract_epitopes(test_file,epitope_file):\n",
    "    epis = np.loadtxt(test_file,usecols=(0),unpack=True,delimiter=',',skiprows=1,comments=None,dtype='str')\n",
    "    epis_u,labels = prep.get_labels(epis)\n",
    "    np.save(epitope_file,epis_u)\n",
    "\n",
    "\n",
    "def make_embeddings(testfile, embed_model):\n",
    "    print(\"start embedding\")\n",
    "    model = retrieve_model().to(device)\n",
    "    # embed_model='embeddings/bert_vdjdb-b-example'#name for the model. Will be used if the model or results are saved\n",
    "    # testfile='./data/vdjdb-b-example.csv' #filename of the used dataset\n",
    "    delimiter=',' #Column delimiter in dataset file\n",
    "    h_cdr3='CDR3B'#Column name for CDR3 of chain 1 in dataset file\n",
    "    h_long='LongB'#Column name for Long TCR-sequence of chain 1 in dataset file\n",
    "    seqs_per_file=50000#Maximum number of sequences in one embedding file. If there are more sequences, the embeddings will be split into several files.\n",
    "    \n",
    "    # extract and save some embeddings\n",
    "    extract_and_save_embeddings(model, data_f_n=testfile, sequence_col=h_long, cdr3_col=h_cdr3, seqs_per_file=seqs_per_file, emb_name=embed_model,separator=delimiter)\n",
    "    print(\"finished embedding\")\n",
    "\n",
    "def make_embeddings_a(testfile, embed_model):\n",
    "    print(\"start embedding\")\n",
    "    model = retrieve_model().to(device)\n",
    "    # embed_model='embeddings/bert_vdjdb-b-example'#name for the model. Will be used if the model or results are saved\n",
    "    # testfile='./data/vdjdb-b-example.csv' #filename of the used dataset\n",
    "    delimiter=',' #Column delimiter in dataset file\n",
    "    h_cdr3='CDR3A'#Column name for CDR3 of chain 1 in dataset file\n",
    "    h_long='LongA'#Column name for Long TCR-sequence of chain 1 in dataset file\n",
    "    seqs_per_file=50000#Maximum number of sequences in one embedding file. If there are more sequences, the embeddings will be split into several files.\n",
    "    \n",
    "    # extract and save some embeddings\n",
    "    extract_and_save_embeddings(model, data_f_n=testfile, sequence_col=h_long, cdr3_col=h_cdr3, seqs_per_file=seqs_per_file, emb_name=embed_model,separator=delimiter)\n",
    "    print(\"finished embedding\")\n",
    "    \n",
    "def Retraining_model_prediction(dataPath='./data/vdjdb-b-example.csv', mode= 'prediction',\\\n",
    "               epitope_labels= './data/unique_epitopes_vdjdb-b-example.npy', \\\n",
    "               chains='AB', h_cdr31='CDR3B', h_long1='LongB',  h_cdr32='CDR3A', h_long2='LongA',  h_v1='VB', h_j1='JB',h_v2='VA', h_j2='JA', model_file='./model/statedict_vdjdb-b-small.pt',  \\\n",
    "               embedfile1= 'embeddings/bert_vdjdb-b-example_0.bin',  embedfile2= 'embeddings/bert_vdjdb-b-example_0.bin',\\\n",
    "               predfile='outputs/preds-b-example.csv', result_path='outputs/b-example_probability.csv', batch_size=256,testfile=None):\n",
    "\n",
    "    p = {      \n",
    "        'dataset': dataPath,\n",
    "        'chains': chains,\n",
    "        'epitope_labels': epitope_labels,\n",
    "        'embedtype': 'cdr3+context',\n",
    "        'h_cdr31': h_cdr31,\n",
    "        'h_long1': h_long1,\n",
    "        'embedfile1': embedfile1,\n",
    "        'h_cdr32':h_cdr32,\n",
    "        'h_long2':h_long2,    \n",
    "        'embedfile2': embedfile2,\n",
    "        'delimiter': ',',\n",
    "        \n",
    "        'mode': mode, \n",
    "        # 'name': modelName,\n",
    "        'folds': 'None',\n",
    "        'fold_num': 0,  \n",
    "        'h_epitope': 'Epitope',\n",
    "\n",
    "        'batch_size': 512,\n",
    "        'betas': [0.9, 0.999],\n",
    "        'use_pos_weight': True,\n",
    "        'dropouts': [0.1, 0.1],\n",
    "        'iters_adam': 2500,\n",
    "        'lr_conv': 0.0002,\n",
    "        'lr_linear': 0.01,\n",
    "        'T_anneal': 3000,\n",
    "        'iters_swa': 500,\n",
    "        'anneal_strategy': 'cos',\n",
    "        'lr_swa': 0.0001,\n",
    "        'T_anneal_swa': 300,\n",
    "        \n",
    "        'resultfile': './outputs/results.tsv',\n",
    "        'print_every': 100,\n",
    "        'lossfile': './outputs/loss_train.tsv',\n",
    "        'lossfile_test': './outputs/loss_test.tsv',\n",
    "        # 'model_folder': model_folder,\n",
    "        'params_to_print': ['fold_num'],\n",
    "\n",
    "        'binary': False,\n",
    "        'binary_label': None,\n",
    "        \n",
    "        'append_oh': False,\n",
    "        'kernel_sizes': [5,9,15,21,3],\n",
    "        'pool': 'max',\n",
    "        'h_v1': h_v1,\n",
    "        'h_j1': h_j1,\n",
    "        'h_v2': h_v2,\n",
    "        'h_j2': h_j2,\n",
    "        'h_nt1': 'none',\n",
    "        'h_nt2': 'none',\n",
    "        'guess_allele01': True,\n",
    "        'model_file': model_file,  \n",
    "        'use_LM': False,\n",
    "        'num_features': 1024,\n",
    "        'input_type': 'tcr+cdr3',\n",
    "        'predfile': predfile,  \n",
    "        'additional_columns': [],\n",
    "        'decimals': 4,\n",
    "        \n",
    "        }\n",
    "\n",
    "    p['two_chains']=len(p['chains'])>1\n",
    "    p['table']=(p['params_to_print'],[p[h] for h in p['params_to_print']])\n",
    "    p['save_intermediate'] = p['lossfile'].lower()!='none' or p['lossfile_test'].lower()!='none'\n",
    "    \n",
    "    data = read_csv(p['dataset'],delimiter=p['delimiter'],dtype=str,keep_default_na=False)\n",
    "    epis_u = np.load(p['epitope_labels'])\n",
    "    n_labels=len(epis_u)\n",
    "    n_chains=len(p['chains'])\n",
    "    \n",
    "    resfile = p['predfile']\n",
    "    with open(resfile,'w') as f:\n",
    "        f.write(p['delimiter'].join(['TCR'+c for c in p['chains']]\n",
    "                +p['additional_columns']+list(epis_u))+'\\n')\n",
    "    \n",
    "    # Load TCRconv model\n",
    "    model = utils.load_model(p['model_file'],p,p['num_features'],n_labels,device)\n",
    "    # embeddings / embedding-models\n",
    "    if p['use_LM']:\n",
    "        LM=retrieve_model().to(device)\n",
    "    \n",
    "    # Get requested sequences for genes\n",
    "    geneseqs={}\n",
    "    for ic,chain in enumerate(p['chains']):\n",
    "        c=str(ic+1)\n",
    "        if p['input_type']=='cdr3+nt':\n",
    "            geneseqs['protV'+c],geneseqs['protJ'+c],geneseqs['ntV'+c],geneseqs['ntJ'+c] = \\\n",
    "                    get_protseqs_ntseqs(chain=chain)\n",
    "        elif p['input_type']=='cdr3+vj':\n",
    "            geneseqs['protV'+c],geneseqs['protJ'+c],_,_ = get_protseqs_ntseqs(chain=chain)\n",
    "    \n",
    "    I=[]\n",
    "    ts_all = [[] for c in p['chains']] # separate list for each chain\n",
    "    icount,i0 = 0,0\n",
    "    imax=len(data)-1\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "    \n",
    "        if p['input_type']=='cdr3+nt':\n",
    "            tcr12 = []\n",
    "            for ic in range(n_chains):\n",
    "                c=str(ic+1)\n",
    "                t,_,_ = determine_tcr_seq_nt(data[p['h_nt'+c]][i],data[p['h_cdr3'+c]][i],geneseqs['protV'+c],\n",
    "                        geneseqs['protJ'+c],geneseqs['ntV'+c],geneseqs['ntJ'+c],guess01=p['guess_allele01'])\n",
    "                tcr12.append(t)\n",
    "        elif p['input_type']=='cdr3+vj':\n",
    "            tcr12 = []\n",
    "            for ic in range(n_chains):\n",
    "                c=str(ic+1)\n",
    "                t = determine_tcr_seq_vj(data[p['h_cdr3'+c]][i],data[p['h_v'+c]][i],data[p['h_j'+c]][i],\n",
    "                    geneseqs['protV'+c],geneseqs['protJ'+c],guess01=p['guess_allele01'])\n",
    "                tcr12.append(t)\n",
    "        elif p['input_type']=='cdr3':\n",
    "            tcr12 = [data[p['h_cdr3'+str(ic+1)]][i] for ic in range(n_chains)]\n",
    "        else: # tcr+cdr3 / tcr\n",
    "            tcr12 = [data[p['h_long'+str(ic+1)]][i] for ic in range(n_chains)]\n",
    "    \n",
    "    \n",
    "        # Check if (either) sequence is empty\n",
    "        if np.any([t=='' for t in tcr12]):\n",
    "            I.append(False)\n",
    "            for ic in range(n_chains):\n",
    "                ts_all[ic].append(tcr12[ic])\n",
    "        else:\n",
    "            I.append(True)\n",
    "            for ic in range(n_chains):\n",
    "                ts_all[ic].append(tcr12[ic])\n",
    "            icount+=1\n",
    "    \n",
    "    \n",
    "        if icount==p['batch_size'] or i==imax:\n",
    "            ts_all =[np.array(t) for t in ts_all]\n",
    "            if icount>0:\n",
    "                I= np.array(I,dtype=bool)\n",
    "                if p['use_LM']: # If LM is used, compute embeddings\n",
    "                    cdr3s = data[p['h_cdr31']][i0:i+1][I].values\n",
    "                    print('computing embeddings: {:d}-{:d}/{:d}'.format(i0,i,imax))\n",
    "                    if p['embedtype']=='cdr3+context':\n",
    "                        embeddings1 = compute_embs(LM, ts_all[0][I], cdr3s)\n",
    "                        embeddings1 = utils.stack_embeddings(embeddings1,device,p['append_oh'],cdr3s)\n",
    "                    else:\n",
    "                        embeddings1 = compute_embs(LM, ts_all[0][I], None)\n",
    "                        embeddings1 = utils.stack_embeddings(embeddings1,device,p['append_oh'])\n",
    "    \n",
    "                    if p['two_chains']:\n",
    "                        cdr3s = data[p['h_cdr32']][i0:i+1][I].values\n",
    "                        if p['embedtype']=='cdr3+context':\n",
    "                            embeddings2 = compute_embs(LM,ts_all[1][I], cdr3s)\n",
    "                            embeddings2 = utils.stack_embeddings(embeddings2,device,p['append_oh'],cdr3s)\n",
    "                        else:\n",
    "                            embeddings2 = compute_embs(LM, ts_all[1][I], None)\n",
    "                            embeddings2 = utils.stack_embeddings(embeddings2,device,p['append_oh'])\n",
    "    \n",
    "                else: # 1-2 embedding dictionaries are used\n",
    "    \n",
    "                    #print(ts_all[0][I])\n",
    "                    cdr3s = data[p['h_cdr31']][i0:i+1][I].values\n",
    "                    cdr3max = utils.maxlen(cdr3s)\n",
    "                    embeddings1 = utils.get_embeddings(utils.get_embedding_dict(p['embedfile1']),\n",
    "                                cdr3s, ts_all[0][I], cdr3max, device, p['append_oh'])\n",
    "                    if p['two_chains']:\n",
    "                        cdr3s = data[p['h_cdr32']][i0:i+1][I].values\n",
    "                        cdr3max = utils.maxlen(cdr3s)\n",
    "                        embeddings2 = utils.get_embeddings(utils.get_embedding_dict(p['embedfile2']),\n",
    "                                cdr3s,ts_all[1][I], cdr3max, device, p['append_oh'])\n",
    "    \n",
    "                # Predictions\n",
    "                if p['two_chains']:\n",
    "                    output = model(embeddings1,embeddings2).detach().cpu().numpy()\n",
    "                else:\n",
    "                    output = model(embeddings1).detach().cpu().numpy()\n",
    "                output = utils.toprob(output)\n",
    "    \n",
    "                pred_ar = np.ones((len(I),n_labels),dtype=float)*np.nan\n",
    "                pred_ar[I,:]= output\n",
    "    \n",
    "            else: # No proper sequences were found, add fillers\n",
    "                pred_ar = np.ones((len(I),n_labels),dtype=float)*np.nan\n",
    "    \n",
    "            # append results to result file\n",
    "            df = DataFrame(np.concatenate([np.expand_dims(ts_all[i],1) for i in range(len(ts_all))] \\\n",
    "                +[np.expand_dims(data[col].values[i0:i+1],1) for col in p['additional_columns']] \\\n",
    "                +[np.round(pred_ar,p['decimals'])],axis=1))\n",
    "            df.to_csv(resfile,sep=p['delimiter'],mode='a',header=False,index=False)\n",
    "           \n",
    "            I = []\n",
    "            ts_all = [[] for c in p['chains']]\n",
    "            icount = 0\n",
    "            i0 = i+1  \n",
    "            \n",
    "    #process result file\n",
    "    df_result=pd.read_csv(resfile)\n",
    "    probability = df_result[['TCRB']]\n",
    "    \n",
    "    df_ori=pd.read_csv(dataPath)\n",
    "    print('df_ori',df_ori)\n",
    "    for k in range(len(df_ori)):\n",
    "        probability.loc[probability.TCRB==df_ori['LongA'][k],'Epitope']=df_ori['Epitope'][k]\n",
    "    print(probability)\n",
    "    get_label=df_result[df_result.columns[1:]]\n",
    "    get_label = get_label.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    pre_epi_ls=get_label.idxmax(axis=1).tolist()   \n",
    "    probability['Epitope_pred']=pre_epi_ls\n",
    "    \n",
    "\n",
    "    get_label['y_prob']=get_label.max(axis=1)\n",
    "    probability['y_prob']=get_label['y_prob'].tolist()\n",
    "    probability['y_pred']=1\n",
    "    for k in range(len(probability)):\n",
    "        if probability['Epitope_pred'][k]!=probability['Epitope'][k]:\n",
    "            probability['y_prob'][k]=0\n",
    "            probability['y_pred'][k]=0\n",
    "\n",
    "\n",
    "    df_tmp=pd.read_csv(testfile)\n",
    "    for i in range(len(df_tmp)):\n",
    "        probability.loc[probability.TCRB==df_tmp['LongA'][i],'y_true']=df_tmp['Affinity'][i]\n",
    "        \n",
    "    print(probability)\n",
    "    probability.to_csv(result_path+'probability.csv',index=False)\n",
    "    print('done saving!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b1c51-65e8-4e06-a5f1-a80d4dada91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='TCRconv-fullA'\n",
    "\n",
    "testfile_path =\"../data/test_CDR3B_others.csv\"\n",
    "file_out=\"../data/TCRconv-A/\"\n",
    "os.makedirs(file_out, exist_ok=True)\n",
    "\n",
    "trainfile_new=file_out+\"train_CDR3B_others.csv\"\n",
    "testfile_new=file_out+\"test_CDR3B_others.csv\"\n",
    "fix(testfile_path,testfile_new)\n",
    "trainfile_out=\"../data/TCRconv-A/\"\n",
    "epitope_file=trainfile_out+'train_unique_epitopes.npy'\n",
    "\n",
    "embed_model='./embeddings/test/TCRconv_fullA/bert'\n",
    "os.makedirs('./embeddings/test/TCRconv_fullA/bert', exist_ok=True)\n",
    "make_embeddings(testfile_new, embed_model)\n",
    "save_model_path= \"../Retraining_model/Retraining_model.pt\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "os.makedirs(result_path,exist_ok=True)\n",
    "initial_result\"../result_path/Retraining_model_prediction/test.csv\"\n",
    "Retraining_model_prediction(dataPath=testfile_new, mode= 'prediction',\\\n",
    "           epitope_labels= epitope_file, \\\n",
    "           chains='A', h_cdr31='CDR3A', h_long1='LongA',  h_v1='VA', h_j1='JA',  model_file=save_model_path, \\\n",
    "           embedfile1= embed_model+'_0.bin',\\\n",
    "           predfile=initial_result, result_path= result_path, batch_size=256, testfile=testfile_path\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e6c19-0a7b-46df-addb-7f549aec59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='TCRconv-fullA'\n",
    "testfile_path =\"../data/Validation_CDR3B_others.csv\"\n",
    "file_out=\"../data/TCRconv-A/\"\n",
    "os.makedirs(file_out, exist_ok=True)\n",
    "\n",
    "trainfile_new=file_out+\"train_CDR3B_others.csv\"\n",
    "testfile_new=file_out+\"Validation_CDR3B_others.csv\"\n",
    "fix(testfile_path,testfile_new)\n",
    "trainfile_out=\"../data/TCRconv-A/\"\n",
    "epitope_file=trainfile_out+'train_unique_epitopes.npy'\n",
    "\n",
    "embed_model='./embeddings/Validation/TCRconv_fullA/bert'\n",
    "os.makedirs('./embeddings/Validation/TCRconv_fullA/bert', exist_ok=True)\n",
    "make_embeddings(testfile_new, embed_model)\n",
    "save_model_path= \"../Retraining_model/Retraining_model.pt\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "os.makedirs(result_path,exist_ok=True)\n",
    "initial_result\"../result_path/Retraining_model_prediction/Validation.csv\"\n",
    "Retraining_model_prediction(dataPath=testfile_new, mode= 'prediction',\\\n",
    "           epitope_labels= epitope_file, \\\n",
    "           chains='A', h_cdr31='CDR3A', h_long1='LongA',  h_v1='VA', h_j1='JA',  model_file=save_model_path, \\\n",
    "           embedfile1= embed_model+'_0.bin',\\\n",
    "           predfile=initial_result, result_path= result_path, batch_size=256, testfile=testfile_path\n",
    "           )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCREpi(tcrconv1)",
   "language": "python",
   "name": "tcrconv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
