{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d282a0",
   "metadata": {},
   "source": [
    "# 1.Original model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3730e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import argparse\n",
    "import ae_utils as ae\n",
    "import lstm_utils as lstm\n",
    "import ergo_data_loader\n",
    "import numpy as np\n",
    "from ERGO_models import AutoencoderLSTMClassifier, DoubleLSTMClassifier\n",
    "import csv\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def Original_model_prediction(args):\n",
    "    import pandas as pd\n",
    "    # Word to index dictionary\n",
    "    amino_acids = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "    if args['model_type'] == 'lstm':\n",
    "        amino_to_ix = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "    if args['model_type'] == 'ae':\n",
    "        pep_atox = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "        tcr_atox = {amino: index for index, amino in enumerate(amino_acids + ['X'])}\n",
    "\n",
    "    # if args.ae_file == 'auto':\n",
    "    args['ae_file'] = 'TCR_Autoencoder/tcr_ae_dim_100.pt'\n",
    "    if args['model_file'] == 'auto':\n",
    "        dir = 'models'\n",
    "        p_key = 'protein' if args.protein else ''\n",
    "        args.model_file = dir + '/' + '_'.join([args['model_type'], args.dataset, args.sampling, p_key, 'model.pt'])\n",
    "    if args['test_data_file'] == 'auto':\n",
    "        args['test_data_file'] = 'pairs_example.csv'\n",
    "\n",
    "    # Read test data\n",
    "    tcrs = []\n",
    "    peps = []\n",
    "    y_true=[]\n",
    "    signs = []\n",
    "    max_len = 28\n",
    "    with open(args['test_data_file'], 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for line in reader:\n",
    "            tcr, pep,lable = line\n",
    "            if args['model_type'] == 'ae' and len(tcr) >= max_len:\n",
    "                continue\n",
    "            tcrs.append(tcr)\n",
    "            peps.append(pep)\n",
    "            y_true.append(lable)\n",
    "            signs.append(0.0)\n",
    "            \n",
    "    tcrs_copy = tcrs.copy()\n",
    "    peps_copy = peps.copy()\n",
    "    y_true_copy = y_true.copy()\n",
    "\n",
    "    # Load model\n",
    "    device = args['device']\n",
    "    if args['model_type'] == 'ae':\n",
    "        model = AutoencoderLSTMClassifier(10, device, 28, 21, 100, 50, args['ae_file'], False)\n",
    "        checkpoint = torch.load(args['model_file'], map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    if args['model_type'] == 'lstm':\n",
    "        model = DoubleLSTMClassifier(10, 500, 0.1, device)\n",
    "        checkpoint = torch.load(args['model_file'], map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        pass\n",
    "\n",
    "    # Predict\n",
    "    batch_size = 50\n",
    "    if args['model_type'] == 'ae':\n",
    "        test_batches = ae.get_full_batches(tcrs, peps, signs, tcr_atox, pep_atox, batch_size, max_len)\n",
    "        preds = ae.predict(model, test_batches, device)\n",
    "      \n",
    "    if args['model_type'] == 'lstm':\n",
    "        lstm.convert_data(tcrs, peps, amino_to_ix)\n",
    "        test_batches = lstm.get_full_batches(tcrs, peps, signs, batch_size, amino_to_ix)\n",
    "        preds = lstm.predict(model, test_batches, device)\n",
    "\n",
    "    data = pd.DataFrame(columns=['CDR3B', 'Class', 'y_true', 'y_prob'])\n",
    "\n",
    "    data_list = []  \n",
    "    for tcr, pep, y_true, pred in zip(tcrs_copy, peps_copy, y_true_copy, preds):\n",
    "        data_list.append({'CDR3B': tcr, 'Class': pep, 'y_true': y_true, 'y_prob': pred})\n",
    "    data = pd.concat([data, pd.DataFrame(data_list)], ignore_index=True)\n",
    "    data['y_pred'] = data['y_prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "    result_path = args['result_path']\n",
    "    data.to_csv(result_path + '_probability.csv', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddb97",
   "metadata": {},
   "source": [
    "When training and predicting with the model, \n",
    "it is necessary to ensure that the column order in the data is \n",
    "['CDR3B', 'Epitope', 'Affinity'], \n",
    "and there should be no extra rows or mismatched column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a1d6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile=pd.read_csv(\"../data/test.csv\")\n",
    "testfile=testfile[['CDR3B','Epitope','Affinity']]\n",
    "testfile.to_csv(\"../data/ERGO_test.csv\",header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'test_data_file':\"../data/ERGO_test.csv\",\n",
    "    'model_type': \"ae\",\n",
    "    'model_file':\"../Original_model/ERGO_AE_mc.pt\",\n",
    "    'result_path':\"../result_path/Original_model_prediction\",\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "\n",
    "Original_model_prediction(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef8e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'test_data_file':\"../data/ERGO_test.csv\",\n",
    "    'model_type': \"lstm\",\n",
    "    'model_file':\"../Original_model/ERGO_lstm_mc.pt\",\n",
    "    'result_path':\"../result_path/Original_model_prediction\",\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "\n",
    "Original_model_prediction(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a12f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478374b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3b048c",
   "metadata": {},
   "source": [
    "# 2.Model retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6706e28",
   "metadata": {},
   "source": [
    "# lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab847f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sklearn.model_selection as skl\n",
    "\n",
    "def lstm_get_lists_from_pairs(pairs):\n",
    "    tcrs = []\n",
    "    peps = []\n",
    "    signs = []\n",
    "    for pair in pairs:\n",
    "        tcr, pep, label = pair\n",
    "        tcrs.append(tcr)\n",
    "        peps.append(pep[0])\n",
    "        if label == 'p':\n",
    "            signs.append(1.0)\n",
    "        elif label == 'n':\n",
    "            signs.append(0.0)\n",
    "    return tcrs, peps, signs\n",
    "\n",
    "def train_epoch(batches, model, loss_function, optimizer, device):\n",
    "    model.train()\n",
    "    shuffle(batches)\n",
    "    total_loss = 0\n",
    "    for batch in batches:\n",
    "        padded_tcrs, tcr_lens, padded_peps, pep_lens, batch_signs = batch\n",
    "        # Move to GPU\n",
    "        padded_tcrs = padded_tcrs.to(device)\n",
    "        tcr_lens = tcr_lens.to(device)\n",
    "        padded_peps = padded_peps.to(device)\n",
    "        pep_lens = pep_lens.to(device)\n",
    "        batch_signs = torch.tensor(batch_signs).to(device)\n",
    "        model.zero_grad()\n",
    "        probs = model(padded_tcrs, tcr_lens, padded_peps, pep_lens)\n",
    "        probs = probs.squeeze()\n",
    "        batch_signs = batch_signs.squeeze()\n",
    "        # Compute loss\n",
    "        weights = batch_signs * 0.84 + (1-batch_signs) * 0.14\n",
    "        loss_function.weight = weights\n",
    "        batch_signs = batch_signs.squeeze()\n",
    "        loss = loss_function(probs, batch_signs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def converted_seq(test_tcrs, test_peps, amino_to_ix):\n",
    "    encoded_tcrs = []\n",
    "    encoded_peps = []\n",
    "    for tcr in test_tcrs:\n",
    "        encoded_tcr = []\n",
    "        for value in tcr:\n",
    "            if value == 0: \n",
    "                continue\n",
    "            for amino, encoding in amino_to_ix.items():\n",
    "                if encoding == value:\n",
    "                    encoded_tcr.append(amino)\n",
    "                    break\n",
    "        encoded_tcrs.append(encoded_tcr)\n",
    "    for pep in test_peps:\n",
    "        encoded_pep = []\n",
    "        for value in pep:\n",
    "            if value == 0:  \n",
    "                continue\n",
    "            for amino, encoding in amino_to_ix.items():\n",
    "                if encoding == value:\n",
    "                    encoded_pep.append(amino)\n",
    "                    break\n",
    "        encoded_peps.append(encoded_pep)\n",
    "    encoded_tcrs = [\"\".join(sequence) for sequence in encoded_tcrs]\n",
    "    encoded_peps = [\"\".join(sequence) for sequence in encoded_peps]\n",
    "    return encoded_tcrs, encoded_peps\n",
    "\n",
    "def evaluate(model, batches, device):\n",
    "    model.eval()\n",
    "    true = []\n",
    "    scores = []\n",
    "    shuffle(batches)\n",
    "    for batch in batches:\n",
    "        padded_tcrs, tcr_lens, padded_peps, pep_lens, batch_signs = batch\n",
    "        padded_tcrs = padded_tcrs.to(device)\n",
    "        tcr_lens = tcr_lens.to(device)\n",
    "        padded_peps = padded_peps.to(device)\n",
    "        pep_lens = pep_lens.to(device)\n",
    "        probs = model(padded_tcrs, tcr_lens, padded_peps, pep_lens)\n",
    "        true.extend(np.array(batch_signs).astype(int))\n",
    "        scores.extend(probs.cpu().data.numpy())\n",
    "    auc = roc_auc_score(true, scores)\n",
    "    fpr, tpr, thresholds = roc_curve(true, scores)\n",
    "    return auc, (fpr, tpr, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84eb0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import lstm_utils as lstm\n",
    "from ERGO_models import AutoencoderLSTMClassifier, DoubleLSTMClassifier\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import pandas as pd\n",
    "\n",
    "def Model_retraining(trainfile_path,testfile_path,save_model_path,result_path): \n",
    "    import pandas as pd\n",
    "    train=pd.read_csv(trainfile_path)\n",
    "    train=train[['CDR3B','Epitope','Affinity']]\n",
    "    train['Affinity'] = train['Affinity'].apply(lambda x: 'p' if x == 1 else 'n')\n",
    "    train = train.values.tolist()\n",
    "    train=[(item[0], (item[1],), item[2]) for item in train]\n",
    "\n",
    "    test=pd.read_csv(testfile_path)\n",
    "    test=test[['CDR3B','Epitope','Affinity']]\n",
    "    test['Affinity'] = test['Affinity'].apply(lambda x: 'p' if x == 1 else 'n')\n",
    "    test = test.values.tolist()\n",
    "    test=[(item[0], (item[1],), item[2]) for item in test]\n",
    "    \n",
    "    amino_acids = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "    amino_to_ix = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "\n",
    "    train_tcrs, train_peps, train_signs = lstm_get_lists_from_pairs(train)\n",
    "    lstm.convert_data(train_tcrs, train_peps, amino_to_ix)\n",
    "    batch_size= 50\n",
    "    train_batches = lstm.get_batches(train_tcrs, train_peps, train_signs, batch_size)\n",
    "\n",
    "    test_tcrs, test_peps, test_signs = lstm_get_lists_from_pairs(test)\n",
    "    lstm.convert_data(test_tcrs, test_peps, amino_to_ix)\n",
    "    test_batches = lstm.get_batches(test_tcrs, test_peps, test_signs, batch_size)\n",
    "    emb_dim=10\n",
    "    lstm_dim=500\n",
    "    dropout=0.1\n",
    "    lr=1e-4\n",
    "    wd=0\n",
    "    option=0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DoubleLSTMClassifier(emb_dim,lstm_dim, dropout, device)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    losses = []\n",
    "    loss_function = nn.BCELoss()\n",
    "    epochs=30\n",
    "    best_auc = 0\n",
    "    best_roc = None\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch:', epoch + 1)\n",
    "        epoch_time = time.time()\n",
    "        loss = train_epoch(train_batches, model, loss_function, optimizer, device)\n",
    "        losses.append(loss)\n",
    "        train_auc = evaluate(model, train_batches, device)[0]\n",
    "        print('train auc:', train_auc)\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        model.eval()\n",
    "        true = []\n",
    "        scores = []\n",
    "        test_tcrs = []\n",
    "        test_peps = []\n",
    "        shuffle(test_batches)\n",
    "        for batch in test_batches: \n",
    "            padded_tcrs, tcr_lens, padded_peps, pep_lens, batch_signs = batch\n",
    "            padded_tcrs = padded_tcrs.to(device)\n",
    "            tcr_lens = tcr_lens.to(device)\n",
    "            padded_peps = padded_peps.to(device)\n",
    "            pep_lens = pep_lens.to(device)\n",
    "            test_tcrs.extend(padded_tcrs.cpu().numpy())  \n",
    "            test_peps.extend(padded_peps.cpu().numpy()) \n",
    "            probs = model(padded_tcrs, tcr_lens, padded_peps, pep_lens)\n",
    "            true.extend(np.array(batch_signs).astype(int))\n",
    "            scores.extend(probs.cpu().data.numpy())\n",
    "        test_auc = roc_auc_score(true, scores)\n",
    "        fpr, tpr, thresholds = roc_curve(true, scores)\n",
    "        tcrs,peps=converted_seq(test_tcrs,test_peps,amino_to_ix)\n",
    "        probability = pd.DataFrame({ 'Epitope': peps, 'CDR3B': tcrs, 'y_true': true, 'y_prob': scores})\n",
    "        probability['y_prob'] = probability['y_prob'].astype(str)\n",
    "        probability['y_prob'] = probability['y_prob'].str.replace('[', '').str.replace(']', '')\n",
    "        probability['y_prob'] = probability['y_prob'].astype(float)\n",
    "        probability['y_pred'] = probability['y_prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        probability.to_csv(result_path+'lstm_probability.csv', index=False)\n",
    "        if test_auc > best_auc:\n",
    "            best_auc = test_auc\n",
    "        print('test auc:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile_path =\"../data/train.csv\"\n",
    "testfile_path=\"../data/test.csv\"\n",
    "save_modle_path=\"../Retraining_model/Retraining_lstm_model.pth\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "Model_retraining(trainfile_path,testfile_path,save_modle_path,result_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c796eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fd33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45214dd5",
   "metadata": {},
   "source": [
    "# 3.Retraining_model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff53e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Retraining_model_prediction(testfile_path,modelfile_path,result_path):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, matthews_corrcoef, precision_score,auc,roc_curve\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    emb_dim=10\n",
    "    lstm_dim=500\n",
    "    dropout=0.1\n",
    "    batch_size= 50\n",
    "    model = DoubleLSTMClassifier(emb_dim, lstm_dim, dropout, device)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(modelfile_path))\n",
    "    model.eval()\n",
    "    test=pd.read_csv(testfile_path)\n",
    "    test=test[['CDR3B','Epitope','Affinity']]\n",
    "    test['Affinity'] = test['Affinity'].apply(lambda x: 'p' if x == 1 else 'n')\n",
    "    test = test.values.tolist()\n",
    "    test=[(item[0], (item[1],), item[2]) for item in test]\n",
    "    amino_acids = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "    amino_to_ix = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "    test_tcrs, test_peps, test_signs = lstm_get_lists_from_pairs(test)\n",
    "    lstm.convert_data(test_tcrs, test_peps, amino_to_ix)\n",
    "    test_batches = lstm.get_batches(test_tcrs, test_peps, test_signs, batch_size)\n",
    "    true = []\n",
    "    scores = []\n",
    "    test_tcrs = []\n",
    "    test_peps = []\n",
    "    for batch in test_batches:\n",
    "        padded_tcrs, tcr_lens, padded_peps, pep_lens, batch_signs = batch\n",
    "        padded_tcrs = padded_tcrs.to(device)\n",
    "        tcr_lens = tcr_lens.to(device)\n",
    "        padded_peps = padded_peps.to(device)\n",
    "        pep_lens = pep_lens.to(device)\n",
    "        test_tcrs.extend(padded_tcrs.cpu().numpy())  \n",
    "        test_peps.extend(padded_peps.cpu().numpy()) \n",
    "        probs = model(padded_tcrs, tcr_lens, padded_peps, pep_lens)\n",
    "        true.extend(np.array(batch_signs).astype(int))\n",
    "        scores.extend(probs.cpu().data.numpy())\n",
    "        tcrs, peps = converted_seq(test_tcrs, test_peps, amino_to_ix)\n",
    "        probability = pd.DataFrame({'Epitope': peps, 'CDR3B': tcrs, 'y_true': true, 'y_prob': scores})\n",
    "        probability['y_prob'] = probability['y_prob'].astype(str)\n",
    "        probability['y_prob'] = probability['y_prob'].str.replace('[', '').str.replace(']', '')\n",
    "        probability['y_prob'] = probability['y_prob'].astype(float)\n",
    "        probability['y_pred'] = probability['y_prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        probability.to_csv(result_path+'lstm_probability.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile_path=\"./data/Validation.csv\"\n",
    "modelfile_path=\"./Retraining_model/Retraining_lstm_model.pth\"\n",
    "result_path=\"./result_path/Retraining_model_prediction\"\n",
    "Retraining_model_prediction(testfile_path,modelfile_path,result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fd56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5977b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17dfa77",
   "metadata": {},
   "source": [
    "# 2.Model_retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d736c0f",
   "metadata": {},
   "source": [
    "# AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc41a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sklearn.model_selection as skl\n",
    "import torch\n",
    "import pickle\n",
    "import lstm_utils as lstm\n",
    "from ERGO_models import AutoencoderLSTMClassifier, DoubleLSTMClassifier\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import pandas as pd\n",
    "def ae_get_lists_from_pairs(pairs, max_len):\n",
    "    tcrs = []\n",
    "    peps = []\n",
    "    signs = []\n",
    "    for pair in pairs:\n",
    "        tcr, pep, label = pair\n",
    "        if len(tcr) >= max_len:\n",
    "            continue\n",
    "        tcrs.append(tcr)\n",
    "        peps.append(pep[0])\n",
    "        if label == 'p':\n",
    "            signs.append(1.0)\n",
    "        elif label == 'n':\n",
    "            signs.append(0.0)\n",
    "    return tcrs, peps, signs\n",
    "\n",
    "\n",
    "def train_epoch(batches, model, loss_function, optimizer, device):\n",
    "    model.train()\n",
    "    shuffle(batches)\n",
    "    total_loss = 0\n",
    "    for batch in batches:\n",
    "        tcrs, padded_peps, pep_lens, batch_signs = batch\n",
    "        # Move to GPU\n",
    "        # print(tcrs)\n",
    "        tcrs = tcrs.to(device)\n",
    "        padded_peps = padded_peps.to(device)\n",
    "        pep_lens = pep_lens.to(device)\n",
    "        batch_signs = torch.tensor(batch_signs).to(device)\n",
    "        model.zero_grad()\n",
    "        probs = model(tcrs, padded_peps, pep_lens)\n",
    "        # print(probs, batch_signs)\n",
    "        # Compute loss\n",
    "        batch_signs = batch_signs.unsqueeze(1)\n",
    "        loss = loss_function(probs, batch_signs)\n",
    "        # with open(sys.argv[1], 'a+') as loss_file:\n",
    "        #    loss_file.write(str(loss.item()) + '\\n')\n",
    "        # Update model weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "\n",
    "def Sequence_conversion(data):\n",
    "    data['Epitope'] = data['Epitope'].apply(lambda x: [int(num) for num in x.strip('[]').split()])\n",
    "    amino_acids = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "    pep_atox = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "    atox_dict_reversed = {v: k for k, v in pep_atox.items()}\n",
    "    for i in range(len(data)):\n",
    "        data['Epitope'][i] = [atox_dict_reversed[num] for num in data['Epitope'][i]]\n",
    "    data['Epitope'] = data['Epitope'].astype(str)\n",
    "    data['Epitope'] = data['Epitope'].str.replace('[', '').str.replace(']', '')\n",
    "    data['Epitope'] = data['Epitope'].apply(lambda x: ''.join([char for char in x if char.isalnum()]))\n",
    "    data['CDR3B'] = data['CDR3B'].str.replace('.', '')\n",
    "    tcr_list=[]\n",
    "    for j in range(0,len( data['CDR3B'])):\n",
    "        matrix_str = str(data['CDR3B'][j])\n",
    "        matrix_rows = matrix_str.strip('[]').split('\\n')\n",
    "        matrix = []\n",
    "        for row in matrix_rows:\n",
    "            values = [int(val) for val in row.strip().strip('[]').split()]\n",
    "            matrix.append(values)\n",
    "        matrix = np.array(matrix)\n",
    "        nonzero_rows = np.any(matrix != 0, axis=1)\n",
    "        matrix = matrix[nonzero_rows]\n",
    "        amino_acids = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7,\n",
    "                       'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14,\n",
    "                       'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, 'X': 20}\n",
    "        sequence = \"\"\n",
    "        for row in matrix:\n",
    "            index = np.argmax(row)\n",
    "            for amino, idx in amino_acids.items():\n",
    "                if idx == index:\n",
    "                    sequence += amino\n",
    "        tcr_list.append(sequence)\n",
    "    data['CDR3B']=  tcr_list\n",
    "    data['CDR3B'] = data['CDR3B'].apply(lambda x: x[:-1] if x[-1] == 'X' else x)\n",
    "    return data\n",
    "\n",
    "\n",
    "def evaluate(model, batches, device):\n",
    "    model.eval()\n",
    "    true = []\n",
    "    scores = []\n",
    "    shuffle(batches)\n",
    "    for batch in batches:\n",
    "        tcrs, padded_peps, pep_lens, batch_signs = batch\n",
    "        tcrs = torch.tensor(tcrs).to(device)\n",
    "        padded_peps = padded_peps.to(device)\n",
    "        pep_lens = pep_lens.to(device)\n",
    "        probs = model(tcrs, padded_peps, pep_lens)\n",
    "        true.extend(np.array(batch_signs).astype(int))\n",
    "        scores.extend(probs.cpu().data.numpy())\n",
    "    auc = roc_auc_score(true, scores)\n",
    "    fpr, tpr, thresholds = roc_curve(true, scores)\n",
    "    return auc, (fpr, tpr, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d44c002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import ae_utils as ae\n",
    "import lstm_utils as lstm\n",
    "import ergo_data_loader\n",
    "import numpy as np\n",
    "from ERGO_models import AutoencoderLSTMClassifier, DoubleLSTMClassifier\n",
    "import csv\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import pandas as pd\n",
    "import ast\n",
    "def Model_retraining(trainfile_path,testfile_path,save_model_path,result_path): \n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, matthews_corrcoef, precision_score,auc,roc_curve\n",
    "    train=pd.read_csv(trainfile_path)\n",
    "    train=train[['CDR3B','Epitope','Affinity']]\n",
    "    train['Affinity'] = train['Affinity'].apply(lambda x: 'p' if x == 1 else 'n')\n",
    "    train = train.values.tolist()\n",
    "    train=[(item[0], (item[1],), item[2]) for item in train]\n",
    "\n",
    "    test=pd.read_csv(testfile_path)\n",
    "    test=test[['CDR3B','Epitope','Affinity']]\n",
    "    test['Affinity'] = test['Affinity'].apply(lambda x: 'p' if x == 1 else 'n')\n",
    "    test = test.values.tolist()\n",
    "    test=[(item[0], (item[1],), item[2]) for item in test]\n",
    "    emb_dim=10\n",
    "    enc_dim= 100\n",
    "    batch_size=50\n",
    "    train_ae=True\n",
    "    dropout=0.1\n",
    "    lr=1e-4\n",
    "    wd=0\n",
    "    option=0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    amino_acids = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "    pep_atox = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "    tcr_atox = {amino: index for index, amino in enumerate(amino_acids + ['X'])}\n",
    "    ae_file = 'TCR_Autoencoder/tcr_ae_dim_' + str(enc_dim) + '.pt'\n",
    "    checkpoint = torch.load(ae_file, map_location=device)\n",
    "    max_len = checkpoint['max_len']\n",
    "    batch_size = checkpoint['batch_size']\n",
    "\n",
    "    train_tcrs, train_peps, train_signs = ae_get_lists_from_pairs(train, max_len)\n",
    "    train_batches = ae.get_batches(train_tcrs, train_peps, train_signs, tcr_atox, pep_atox, batch_size, max_len)\n",
    "    test_tcrs, test_peps, test_signs = ae_get_lists_from_pairs(test,max_len)\n",
    "    test_batches = ae.get_batches(test_tcrs, test_peps, test_signs, tcr_atox, pep_atox, batch_size,max_len)\n",
    "\n",
    "    losses = []\n",
    "    loss_function = nn.BCELoss()\n",
    "    model = AutoencoderLSTMClassifier(emb_dim, device,max_len, 21, enc_dim, batch_size, ae_file, train_ae)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    best_auc = 0\n",
    "    best_roc = None\n",
    "    epochs=50\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch:', epoch + 1)\n",
    "        epoch_time = time.time()\n",
    "        loss = train_epoch(train_batches,model, loss_function, optimizer, device)\n",
    "        losses.append(loss)\n",
    "        train_auc = evaluate(model, train_batches, device)[0] \n",
    "        print('train auc:', train_auc)\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        model.eval()\n",
    "        true = []\n",
    "        scores = []\n",
    "        test_tcrs = []\n",
    "        test_peps = []\n",
    "        for batch in test_batches: \n",
    "            tcrs, padded_peps, pep_lens, batch_signs = batch\n",
    "            tcrs = torch.tensor(tcrs).to(device)\n",
    "            padded_peps = padded_peps.to(device)\n",
    "            pep_lens = pep_lens.to(device)\n",
    "            test_tcrs.extend(tcrs.cpu().numpy())  \n",
    "            test_peps.extend(padded_peps.cpu().numpy()) \n",
    "            probs = model(tcrs, padded_peps, pep_lens)\n",
    "            true.extend(np.array(batch_signs).astype(int))\n",
    "            scores.extend(probs.cpu().data.numpy())    \n",
    "        \n",
    "        test_auc = roc_auc_score(true, scores)\n",
    "        fpr, tpr, thresholds = roc_curve(true, scores)\n",
    "        probability = pd.DataFrame({ 'Epitope': test_peps, 'CDR3B': test_tcrs, 'y_true': true, 'y_prob': scores})\n",
    "        probability['y_prob'] = probability['y_prob'].astype(str)\n",
    "        probability['y_prob'] = probability['y_prob'].str.replace('[', '').str.replace(']', '')\n",
    "        probability['y_prob'] = probability['y_prob'].astype(float)\n",
    "        probability['y_pred'] = probability['y_prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        probability.to_csv(result_path+'ae_probability.csv', index=False)\n",
    "        probability=pd.read_csv(result_path+'ae_probability.csv')\n",
    "        probability=Sequence_conversion(probability)\n",
    "        probability.to_csv(result_path+'ae_probability.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile_path =\"../data/train.csv\"\n",
    "testfile_path=\"../data/test.csv\"\n",
    "save_modle_path=\"../Retraining_model/Retraining_ae_model.pth\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "Model_retraining(trainfile_path,testfile_path,save_modle_path,result_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37f712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec54be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb845e3",
   "metadata": {},
   "source": [
    "# 3.Retraining_model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f072762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Retraining_model_prediction(testfile_path,modelfile_path,result_path): \n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, matthews_corrcoef, precision_score,auc,roc_curve\n",
    "    test=pd.read_csv(testfile_path)\n",
    "    test=test[['CDR3B','Epitope','Affinity']]\n",
    "    test['Affinity'] = test['Affinity'].apply(lambda x: 'p' if x == 1 else 'n')\n",
    "    test = test.values.tolist()\n",
    "    test=[(item[0], (item[1],), item[2]) for item in test]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    amino_acids = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "    emb_dim=10\n",
    "    enc_dim= 100\n",
    "    batch_size=50\n",
    "    train_ae=True\n",
    "    dropout=0.1\n",
    "    lr=1e-4\n",
    "    wd=0\n",
    "    option=0\n",
    "    pep_atox = {amino: index for index, amino in enumerate(['PAD'] + amino_acids)}\n",
    "    tcr_atox = {amino: index for index, amino in enumerate(amino_acids + ['X'])}\n",
    "    ae_file = 'TCR_Autoencoder/tcr_ae_dim_' + str(enc_dim) + '.pt'\n",
    "    checkpoint = torch.load(ae_file, map_location=device)\n",
    "    max_len = checkpoint['max_len']\n",
    "    batch_size = checkpoint['batch_size']\n",
    "\n",
    "    model = AutoencoderLSTMClassifier(emb_dim, device, max_len, 21, enc_dim, batch_size, ae_file, False)\n",
    "    model.load_state_dict(torch.load(modelfile_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_tcrs, test_peps, test_signs = ae_get_lists_from_pairs(test,max_len)\n",
    "    test_batches = ae.get_batches(test_tcrs, test_peps, test_signs, tcr_atox, pep_atox, batch_size,max_len)\n",
    "\n",
    "    true = []\n",
    "    scores = []\n",
    "    test_tcrs = []\n",
    "    test_peps = []\n",
    "    for batch in test_batches: \n",
    "        tcrs, padded_peps, pep_lens, batch_signs = batch\n",
    "        tcrs = torch.tensor(tcrs).to(device)\n",
    "        padded_peps = padded_peps.to(device)\n",
    "        pep_lens = pep_lens.to(device)\n",
    "        test_tcrs.extend(tcrs.cpu().numpy())  \n",
    "        test_peps.extend(padded_peps.cpu().numpy()) \n",
    "        probs = model(tcrs, padded_peps, pep_lens)\n",
    "        true.extend(np.array(batch_signs).astype(int))\n",
    "        scores.extend(probs.cpu().data.numpy())    \n",
    "    test_auc = roc_auc_score(true, scores)\n",
    "    fpr, tpr, thresholds = roc_curve(true, scores)\n",
    "    probability = pd.DataFrame({ 'Epitope': test_peps, 'CDR3B': test_tcrs, 'y_true': true, 'y_prob': scores})\n",
    "    probability['y_prob'] = probability['y_prob'].astype(str)\n",
    "    probability['y_prob'] = probability['y_prob'].str.replace('[', '').str.replace(']', '')\n",
    "    probability['y_prob'] = probability['y_prob'].astype(float)\n",
    "    probability['y_pred'] = probability['y_prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "    probability.to_csv(result_path+'ae_probability.csv', index=False)\n",
    "    probability=pd.read_csv(result_path+'ae_probability.csv')\n",
    "    probability=Sequence_conversion(probability)\n",
    "    probability.to_csv(result_path+'ae_probability.csv', index=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile_path=\"../data/Validation.csv\"\n",
    "modelfile_path=\"../Retraining_model/Retraining_ae_model.pth\"\n",
    "result_path=\"../result_path/Retraining_model_prediction\"\n",
    "Retraining_model_prediction(testfile_path,modelfile_path,result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d6bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ERGO",
   "language": "python",
   "name": "ergo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
